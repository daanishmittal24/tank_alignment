{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "\n",
    "def load_model(config_file, weights_file):\n",
    "    # Load the configuration\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(config_file)\n",
    "    cfg.MODEL.WEIGHTS = weights_file\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.97\n",
    "      # Set a custom testing threshold\n",
    "    return DefaultPredictor(cfg)\n",
    "\n",
    "def process_image(predictor, image_path):\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "    outputs = predictor(image)\n",
    "    \n",
    "    # Visualize the results\n",
    "    v = Visualizer(image[:, :, ::-1], MetadataCatalog.get(predictor.cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    \n",
    "    # Display the image\n",
    "    result_image = out.get_image()[:, :, ::-1]\n",
    "    cv2.imshow(f'Result for {image_path}', result_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def process_video(predictor, video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        outputs = predictor(frame)\n",
    "        v = Visualizer(frame[:, :, ::-1], MetadataCatalog.get(predictor.cfg.DATASETS.TRAIN[0]), scale=0.4)\n",
    "        out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "        \n",
    "        # Display the frame\n",
    "        result_frame = out.get_image()[:, :, ::-1]\n",
    "        cv2.imshow(f'Result for a frame in {video_path}', result_frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def process_folder(predictor, folder_path):\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            if file_path.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff')):\n",
    "                process_image(predictor, file_path)\n",
    "            elif file_path.lower().endswith(('.mp4', '.avi', '.mov', '.mkv')):\n",
    "                process_video(predictor, file_path)\n",
    "\n",
    "# Example usage\n",
    "config_file = \"C:/Users/Daanish Mittal/OneDrive/Desktop/Tank_align/tank_alignment/army_ramp/loader.ramp/mask_rcnn_R_101_FPN_3x/2024-07-09-01-55-42/config.yaml\"\n",
    "weights_file = \"C:/Users/Daanish Mittal/OneDrive/Desktop/Tank_align/tank_alignment/army_ramp/loader.ramp/mask_rcnn_R_101_FPN_3x/2024-07-09-01-55-42/model_final.pth\"\n",
    "input_folder = \"C:/Users/Daanish Mittal/OneDrive/Desktop/Tank_align/tank_alignment/test_army\"\n",
    "\n",
    "\n",
    "predictor = load_model(config_file, weights_file)\n",
    "process_folder(predictor, input_folder)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T22:48:06.546338Z",
     "start_time": "2024-07-23T22:48:02.485875Z"
    }
   },
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "import numpy as np\n",
    "\n",
    "def load_model(config_file, weights_file):\n",
    "    # Load the configuration\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(config_file)\n",
    "    cfg.MODEL.WEIGHTS = weights_file\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.99  # Set a custom testing threshold\n",
    "    return DefaultPredictor(cfg)\n",
    "\n",
    "def draw_quadrilaterals(image, outputs, deformation_threshold=0.05):\n",
    "    masks = outputs[\"instances\"].pred_masks.to(\"cpu\").numpy()\n",
    "    for mask in masks:\n",
    "        # Find contours\n",
    "        contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        if len(contours) == 0:\n",
    "            continue\n",
    "        \n",
    "        for contour in contours:\n",
    "            # Approximate the contour to a quadrilateral\n",
    "            epsilon = deformation_threshold * cv2.arcLength(contour, True)\n",
    "            approx = cv2.approxPolyDP(contour, epsilon, True)\n",
    "            \n",
    "            # Ensure it's a quadrilateral (4 sides)\n",
    "            if len(approx) == 4:\n",
    "                # Draw the quadrilateral\n",
    "                cv2.polylines(image, [approx], True, (0, 255, 0), 2)\n",
    "                \n",
    "    return image\n",
    "\n",
    "\n",
    "def process_image(predictor, image_path):\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "    outputs = predictor(image)\n",
    "    \n",
    "    # Draw quadrilaterals and masks on the image\n",
    "    result_image = draw_quadrilaterals(image, outputs)\n",
    "    \n",
    "    # Display the image\n",
    "    cv2.imshow(f'Result for {image_path}', result_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def process_video(predictor, video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        outputs = predictor(frame)\n",
    "        result_frame = draw_quadrilaterals(frame, outputs)\n",
    "        \n",
    "        # Display the frame\n",
    "        cv2.imshow(f'Result for a frame in {video_path}', result_frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def process_folder(predictor, folder_path):\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            if file_path.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff')):\n",
    "                process_image(predictor, file_path)\n",
    "            elif file_path.lower().endswith(('.mp4', '.avi', '.mov', '.mkv')):\n",
    "                process_video(predictor, file_path)\n",
    "\n",
    "# Example usage\n",
    "config_file = \"C:/Users/Daanish Mittal/OneDrive/Desktop/Tank_align/tank_alignment/army_ramp/loader.ramp/mask_rcnn_R_101_FPN_3x/2024-07-09-01-55-42/config.yaml\"\n",
    "weights_file = \"C:/Users/Daanish Mittal/OneDrive/Desktop/Tank_align/tank_alignment/army_ramp/loader.ramp/mask_rcnn_R_101_FPN_3x/2024-07-09-01-55-42/model_final.pth\"\n",
    "input_folder = \"C:/Users/Daanish Mittal/OneDrive/Desktop/Tank_align/tank_alignment/test_army\"\n",
    "\n",
    "predictor = load_model(config_file, weights_file)\n",
    "process_folder(predictor, input_folder)\n"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Calculated Code**"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T22:56:42.347248Z",
     "start_time": "2024-07-23T22:56:07.507463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "\n",
    "def load_model(config_file, weights_file):\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(config_file)\n",
    "    cfg.MODEL.WEIGHTS = weights_file\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.99\n",
    "    return DefaultPredictor(cfg)\n",
    "\n",
    "def calculate_polygon_angle(contour):\n",
    "    rect = cv2.minAreaRect(contour)\n",
    "    angle = rect[2]\n",
    "    if angle < -45:\n",
    "        angle = 90 + angle\n",
    "    elif angle > 45:\n",
    "        angle = angle - 90\n",
    "    else:\n",
    "        angle = 90 - abs(angle)\n",
    "    return angle\n",
    "\n",
    "def mask_sides(image, mask_percentage=0.2):\n",
    "    h, w = image.shape[:2]\n",
    "    mask_width = int(w * mask_percentage)\n",
    "    \n",
    "    mask = np.ones(image.shape[:2], dtype=np.uint8)\n",
    "    mask[:, :mask_width] = 0\n",
    "    mask[:, -mask_width:] = 0\n",
    "    \n",
    "    masked_image = image.copy()\n",
    "    masked_image[:, :mask_width] = 0\n",
    "    masked_image[:, -mask_width:] = 0\n",
    "    \n",
    "    return masked_image, mask\n",
    "\n",
    "def point_line_distance(point, line_start, line_end):\n",
    "    line_vec = np.array(line_end) - np.array(line_start)\n",
    "    point_vec = np.array(point) - np.array(line_start)\n",
    "    line_len = np.linalg.norm(line_vec)\n",
    "    line_unitvec = line_vec / line_len\n",
    "    point_vec_scaled = point_vec / line_len\n",
    "    t = np.dot(line_unitvec, point_vec_scaled)\n",
    "    t = np.clip(t, 0, 1)\n",
    "    nearest = line_start + t * line_vec\n",
    "    dist = np.linalg.norm(point - nearest)\n",
    "    return dist\n",
    "\n",
    "def process_frame(predictor, frame, ramp_index):\n",
    "    masked_frame, frame_mask = mask_sides(frame, 0.3)\n",
    "    \n",
    "    outputs = predictor(masked_frame)\n",
    "    instances = outputs[\"instances\"].to(\"cpu\")\n",
    "    ramp_instances = instances[instances.pred_classes == ramp_index]\n",
    "    \n",
    "    draw_frame = masked_frame.copy()\n",
    "    \n",
    "    for i in range(len(ramp_instances)):\n",
    "        if ramp_instances.has(\"pred_masks\"):\n",
    "            mask = ramp_instances.pred_masks[i].numpy()\n",
    "            mask = cv2.bitwise_and(mask.astype(np.uint8), frame_mask)\n",
    "            \n",
    "            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            if contours:\n",
    "                largest_contour = max(contours, key=cv2.contourArea)\n",
    "                \n",
    "                epsilon = 0.05 * cv2.arcLength(largest_contour, True)\n",
    "                approx_contour = cv2.approxPolyDP(largest_contour, epsilon, True)\n",
    "                \n",
    "                if len(approx_contour) > 4:\n",
    "                    approx_contour = cv2.convexHull(approx_contour)\n",
    "                    approx_contour = cv2.approxPolyDP(approx_contour, epsilon, True)\n",
    "                \n",
    "                if len(approx_contour) > 4:\n",
    "                    approx_contour = cv2.approxPolyDP(approx_contour, epsilon * 2, True)\n",
    "                \n",
    "                if len(approx_contour) > 4:\n",
    "                    approx_contour = cv2.convexHull(approx_contour, returnPoints=False)\n",
    "                    approx_contour = cv2.approxPolyDP(approx_contour, epsilon * 3, True)\n",
    "                \n",
    "                approx_contour = approx_contour[:4]\n",
    "\n",
    "                cv2.drawContours(draw_frame, [approx_contour], 0, (128, 0, 128), 2)\n",
    "                \n",
    "                x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "                cv2.rectangle(draw_frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "                \n",
    "                extension_length = 50\n",
    "                extended_edge_top_start = (x, y)\n",
    "                extended_edge_bottom_start = (x, y + h)\n",
    "                extended_edge_top_end = (x, y - extension_length)\n",
    "                extended_edge_bottom_end = (x, y + h + extension_length)\n",
    "                \n",
    "                cv2.line(draw_frame, extended_edge_top_start, extended_edge_bottom_start, (0, 0, 255), 2)\n",
    "                cv2.line(draw_frame, extended_edge_top_start, extended_edge_top_end, (0, 0, 255), 2)\n",
    "                cv2.line(draw_frame, extended_edge_bottom_start, extended_edge_bottom_end, (0, 0, 255), 2)\n",
    "                \n",
    "                ramp_edge_start = tuple(approx_contour[0][0])\n",
    "                ramp_edge_end = tuple(approx_contour[1][0])\n",
    "                \n",
    "                vector_extended = np.array([extended_edge_bottom_end[0] - extended_edge_top_end[0], extended_edge_bottom_end[1] - extended_edge_top_end[1]])\n",
    "                vector_ramp = np.array([ramp_edge_end[0] - ramp_edge_start[0], ramp_edge_end[1] - ramp_edge_start[1]])\n",
    "                \n",
    "                dot_product = np.dot(vector_extended, vector_ramp)\n",
    "                norm_extended = np.linalg.norm(vector_extended)\n",
    "                norm_ramp = np.linalg.norm(vector_ramp)\n",
    "                \n",
    "                cosine_angle = dot_product / (norm_extended * norm_ramp)\n",
    "                angle = np.arccos(cosine_angle)\n",
    "                angle_degrees = np.degrees(angle)\n",
    "                \n",
    "                cv2.putText(draw_frame, f\"Angle: {angle_degrees:.2f}\", (x, y - 10), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 255), 2)\n",
    "                \n",
    "                mid_x = draw_frame.shape[1] // 2\n",
    "                \n",
    "                intersection_point = (mid_x, y + h)\n",
    "                \n",
    "                line_length = 200\n",
    "                end_x = int(intersection_point[0] + line_length * np.sin(np.radians(angle_degrees)))\n",
    "                end_y = int(intersection_point[1] - line_length * np.cos(np.radians(angle_degrees)))\n",
    "                \n",
    "                cv2.line(draw_frame, intersection_point, (end_x, end_y), (0, 255, 0), 2)\n",
    "\n",
    "                left_edge_start = (0, 0)\n",
    "                left_edge_end = (0, draw_frame.shape[0])\n",
    "                \n",
    "                dist = point_line_distance((0, y + h), left_edge_start, left_edge_end)\n",
    "                \n",
    "                if dist < 5:\n",
    "                    tint_color = (0, 255, 0)  # Green\n",
    "                elif dist < 20:\n",
    "                    tint_color = (0, 165, 255)  # Orange\n",
    "                else:\n",
    "                    tint_color = (0, 0, 255)  # Red\n",
    "                \n",
    "                overlay = np.full(draw_frame.shape, tint_color, dtype=np.uint8)\n",
    "                alpha = 0.3\n",
    "                \n",
    "                draw_frame = cv2.addWeighted(overlay, alpha, draw_frame, 1 - alpha, 0)\n",
    "\n",
    "                cv2.line(draw_frame, intersection_point, (end_x, end_y), tint_color, 2)\n",
    "    \n",
    "    return draw_frame\n",
    "\n",
    "def process_image(predictor, image_path, ramp_index):\n",
    "    image = cv2.imread(image_path)\n",
    "    result_image = process_frame(predictor, image, ramp_index)\n",
    "    cv2.imshow(f'Result for {image_path}', result_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def process_video(predictor, video_path, ramp_index, output_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    out = cv2.VideoWriter(output_path, fourcc, 30.0, (frame_width, frame_height))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        result_frame = process_frame(predictor, frame, ramp_index)\n",
    "        out.write(result_frame)\n",
    "        \n",
    "        cv2.imshow(f'Result for {video_path}', result_frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def process_folder(predictor, folder_path, ramp_index):\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            if file_path.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff')):\n",
    "                process_image(predictor, file_path, ramp_index)\n",
    "            elif file_path.lower().endswith(('.mp4', '.avi', '.mov', '.mkv')):\n",
    "                output_path = os.path.join(folder_path, f\"processed_{file_name}\")\n",
    "                process_video(predictor, file_path, ramp_index, output_path)\n",
    "\n",
    "def main():\n",
    "    config_file = \"C:/Users/Daanish Mittal/OneDrive/Desktop/Tank_align/tank_alignment/army_ramp/loader.ramp/mask_rcnn_R_101_FPN_3x/2024-07-09-01-55-42/config.yaml\"\n",
    "    weights_file = \"C:/Users/Daanish Mittal/OneDrive/Desktop/Tank_align/tank_alignment/army_ramp/loader.ramp/mask_rcnn_R_101_FPN_3x/2024-07-09-01-55-42/model_final.pth\"\n",
    "    input_folder = \"C:/Users/Daanish Mittal/OneDrive/Desktop/Tank_align/tank_alignment/test_army\"\n",
    "\n",
    "    predictor = load_model(config_file, weights_file)\n",
    "    \n",
    "    # Set ramp_index directly\n",
    "    ramp_index = 1\n",
    "    \n",
    "    process_folder(predictor, input_folder, ramp_index)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T05:09:33.994009Z",
     "start_time": "2024-07-24T05:09:07.149875Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor\n",
    "\n",
    "def load_model(config_file, weights_file):\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(config_file)\n",
    "    cfg.MODEL.WEIGHTS = weights_file\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.99\n",
    "    return DefaultPredictor(cfg)\n",
    "\n",
    "def mask_sides(image, mask_percentage=0.2):\n",
    "    h, w = image.shape[:2]\n",
    "    mask_width = int(w * mask_percentage)\n",
    "    \n",
    "    mask = np.ones(image.shape[:2], dtype=np.uint8)\n",
    "    mask[:, :mask_width] = 0\n",
    "    mask[:, -mask_width:] = 0\n",
    "    \n",
    "    masked_image = cv2.bitwise_and(image, image, mask=mask)\n",
    "    \n",
    "    return masked_image, mask\n",
    "\n",
    "def tint_screen(image, color):\n",
    "    overlay = np.zeros_like(image)\n",
    "    output = image.copy()\n",
    "    alpha = 0.3  # Transparency factor.\n",
    "    if color == 'green':\n",
    "        overlay[:] = (0, 255, 0)\n",
    "    elif color == 'orange':\n",
    "        overlay[:] = (0, 165, 255)\n",
    "    else:\n",
    "        overlay[:] = (0, 0, 255)\n",
    "    cv2.addWeighted(overlay, alpha, output, 1 - alpha, 0, output)\n",
    "    return output\n",
    "\n",
    "def calculate_angle(vector1, vector2):\n",
    "    # Ensure vectors point upwards\n",
    "    if vector1[1] > 0:\n",
    "        vector1 = -vector1\n",
    "    if vector2[1] > 0:\n",
    "        vector2 = -vector2\n",
    "    \n",
    "    # Calculate unit vectors\n",
    "    unit_vector1 = vector1 / np.linalg.norm(vector1)\n",
    "    unit_vector2 = vector2 / np.linalg.norm(vector2)\n",
    "    \n",
    "    # Calculate dot product and angle\n",
    "    dot_product = np.dot(unit_vector1, unit_vector2)\n",
    "    angle = np.arccos(np.clip(dot_product, -1.0, 1.0))\n",
    "    \n",
    "    return np.degrees(angle)\n",
    "\n",
    "def calculate_distance(point, line_x):\n",
    "    return abs(point[0] - line_x)\n",
    "\n",
    "def process_frame(predictor, frame, ramp_index):\n",
    "    masked_frame, frame_mask = mask_sides(frame, 0.2)\n",
    "    \n",
    "    outputs = predictor(masked_frame)\n",
    "    instances = outputs[\"instances\"].to(\"cpu\")\n",
    "    ramp_instances = instances[instances.pred_classes == ramp_index]\n",
    "    \n",
    "    draw_frame = masked_frame.copy()\n",
    "    \n",
    "    for i in range(len(ramp_instances)):\n",
    "        if ramp_instances.has(\"pred_masks\"):\n",
    "            mask = ramp_instances.pred_masks[i].numpy()\n",
    "            mask = cv2.bitwise_and(mask.astype(np.uint8), frame_mask)\n",
    "            \n",
    "            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            if contours:\n",
    "                largest_contour = max(contours, key=cv2.contourArea)\n",
    "                \n",
    "                epsilon = 0.05 * cv2.arcLength(largest_contour, True)\n",
    "                approx_contour = cv2.approxPolyDP(largest_contour, epsilon, True)\n",
    "                \n",
    "                if len(approx_contour) > 4:\n",
    "                    approx_contour = cv2.convexHull(approx_contour)\n",
    "                    approx_contour = cv2.approxPolyDP(approx_contour, epsilon, True)\n",
    "                \n",
    "                if len(approx_contour) > 4:\n",
    "                    approx_contour = cv2.approxPolyDP(approx_contour, epsilon * 2, True)\n",
    "                \n",
    "                if len(approx_contour) > 4:\n",
    "                    approx_contour = cv2.convexHull(approx_contour)\n",
    "                    approx_contour = cv2.approxPolyDP(approx_contour, epsilon * 3, True)\n",
    "                \n",
    "                approx_contour = approx_contour[:4]\n",
    "\n",
    "                # Find the rightmost edge of the polygon\n",
    "                rightmost_points = sorted(approx_contour, key=lambda x: x[0][0])[-2:]\n",
    "                right_edge_start = tuple(rightmost_points[0][0])\n",
    "                right_edge_end = tuple(rightmost_points[1][0])\n",
    "\n",
    "                # Ensure the edge is drawn from top to bottom\n",
    "                if right_edge_start[1] > right_edge_end[1]:\n",
    "                    right_edge_start, right_edge_end = right_edge_end, right_edge_start\n",
    "\n",
    "                cv2.line(draw_frame, right_edge_start, right_edge_end, (128, 0, 128), 2)\n",
    "                \n",
    "                # Draw the vertical green line in the middle of the frame\n",
    "                mid_x = draw_frame.shape[1] // 2\n",
    "                cv2.line(draw_frame, (mid_x, 0), (mid_x, draw_frame.shape[0]), (0, 255, 0), 2)\n",
    "\n",
    "                # Calculate the vector of the right edge\n",
    "                vector_right_edge = np.array([right_edge_end[0] - right_edge_start[0], right_edge_end[1] - right_edge_start[1]])\n",
    "                \n",
    "                # Calculate the vector of the green line (always vertical)\n",
    "                vector_green_line = np.array([0, -1])  # Pointing upwards\n",
    "\n",
    "                # Calculate the angle between the right edge and the green line\n",
    "                angle_degrees = calculate_angle(vector_right_edge, vector_green_line)\n",
    "\n",
    "                # Calculate the distance between the green line and the closest point on the right edge\n",
    "                distances = [calculate_distance(point[0], mid_x) for point in rightmost_points]\n",
    "                min_distance = min(distances)\n",
    "\n",
    "                # Apply tint based on angle and distance\n",
    "                if angle_degrees < 5:\n",
    "                    if min_distance < 5:\n",
    "                        draw_frame = tint_screen(draw_frame, 'green')\n",
    "                    elif min_distance < 15:\n",
    "                        draw_frame = tint_screen(draw_frame, 'orange')\n",
    "                    else:\n",
    "                        draw_frame = tint_screen(draw_frame, 'red')\n",
    "                elif angle_degrees < 15:\n",
    "                    if min_distance < 15:\n",
    "                        draw_frame = tint_screen(draw_frame, 'orange')\n",
    "                    else:\n",
    "                        draw_frame = tint_screen(draw_frame, 'red')\n",
    "                else:\n",
    "                    draw_frame = tint_screen(draw_frame, 'red')\n",
    "\n",
    "                # Draw angle information\n",
    "                x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "                cv2.putText(draw_frame, f\"Angle: {angle_degrees:.2f}\", (x, y - 10), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 255), 2)\n",
    "    \n",
    "    return draw_frame\n",
    "\n",
    "def process_video(predictor, video_path, output_path, ramp_index):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        result_frame = process_frame(predictor, frame, ramp_index)\n",
    "        out.write(result_frame)\n",
    "        \n",
    "        cv2.imshow('Processed Video', result_frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def process_folder(predictor, input_folder, ramp_index):\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(('.mp4', '.avi', '.mov')):\n",
    "            input_path = os.path.join(input_folder, filename)\n",
    "            output_path = os.path.join(input_folder, f'processed_{filename}')\n",
    "            process_video(predictor, input_path, output_path, ramp_index)\n",
    "\n",
    "def main():\n",
    "    config_file = \"C:/Users/Daanish Mittal/OneDrive/Desktop/Tank_align/tank_alignment/army_ramp/loader.ramp/mask_rcnn_R_101_FPN_3x/2024-07-09-01-55-42/config.yaml\"\n",
    "    weights_file = \"C:/Users/Daanish Mittal/OneDrive/Desktop/Tank_align/tank_alignment/army_ramp/loader.ramp/mask_rcnn_R_101_FPN_3x/2024-07-09-01-55-42/model_final.pth\"\n",
    "    input_folder = \"C:/Users/Daanish Mittal/OneDrive/Desktop/Tank_align/tank_alignment/test_army\"\n",
    "\n",
    "    predictor = load_model(config_file, weights_file)\n",
    "    \n",
    "    # Set ramp_index directly\n",
    "    ramp_index = 1\n",
    "    \n",
    "    process_folder(predictor, input_folder, ramp_index)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:detectron2.checkpoint.detection_checkpoint:[DetectionCheckpointer] Loading from C:/Users/Daanish Mittal/OneDrive/Desktop/Tank_align/tank_alignment/army_ramp/loader.ramp/mask_rcnn_R_101_FPN_3x/2024-07-09-01-55-42/model_final.pth ...\n",
      "INFO:fvcore.common.checkpoint:[Checkpointer] Loading from c:/Users/Daanish Mittal/OneDrive/Desktop/Tank_align/tank_alignment/army_ramp/loader.ramp/mask_rcnn_R_101_FPN_3x/2024-07-09-01-55-42/model_final.pth ...\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "***Ramp with Webcam***  "
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T07:47:31.225521Z",
     "start_time": "2024-07-24T07:46:57.560636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from collections import deque\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "import numpy as np\n",
    "\n",
    "# Constants\n",
    "MASK_PERCENTAGE = 0.2\n",
    "TRANSPARENCY_ALPHA = 0.3\n",
    "GREEN_COLOR = (0, 255, 0)\n",
    "ORANGE_COLOR = (0, 165, 255)\n",
    "RED_COLOR = (0, 0, 255)\n",
    "EDGE_COLOR = (128, 0, 128)\n",
    "LINE_THICKNESS = 2\n",
    "SMOOTHING_EPSILON_FACTOR = 0.05\n",
    "FRAME_HISTORY = 5\n",
    "\n",
    "# Deque to store edge positions\n",
    "edge_history = deque(maxlen=FRAME_HISTORY)\n",
    "\n",
    "def load_model(config_file, weights_file):\n",
    "    print(\"Loading model...\")\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(config_file)\n",
    "    cfg.MODEL.WEIGHTS = weights_file\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.97  # Set a custom testing threshold\n",
    "    print(\"Model loaded successfully.\")\n",
    "    return DefaultPredictor(cfg)\n",
    "\n",
    "def mask_sides(image, mask_percentage):\n",
    "    h, w = image.shape[:2]\n",
    "    mask_width = int(w * mask_percentage)\n",
    "    mask = np.ones(image.shape[:2], dtype=np.uint8)\n",
    "    mask[:, :mask_width] = 0\n",
    "    mask[:, -mask_width:] = 0\n",
    "    masked_image = cv2.bitwise_and(image, image, mask=mask)\n",
    "    return masked_image, mask\n",
    "\n",
    "def tint_screen(image, color, alpha):\n",
    "    overlay = np.zeros_like(image)\n",
    "    if color == 'green':\n",
    "        overlay[:] = GREEN_COLOR\n",
    "    elif color == 'orange':\n",
    "        overlay[:] = ORANGE_COLOR\n",
    "    else:\n",
    "        overlay[:] = RED_COLOR\n",
    "    cv2.addWeighted(overlay, alpha, image, 1 - alpha, 0, image)\n",
    "    return image\n",
    "\n",
    "def calculate_angle(vector1, vector2):\n",
    "    if vector1[1] > 0:\n",
    "        vector1 = -vector1\n",
    "    if vector2[1] > 0:\n",
    "        vector2 = -vector2\n",
    "    unit_vector1 = vector1 / np.linalg.norm(vector1)\n",
    "    unit_vector2 = vector2 / np.linalg.norm(vector2)\n",
    "    dot_product = np.dot(unit_vector1, unit_vector2)\n",
    "    angle = np.arccos(np.clip(dot_product, -1.0, 1.0))\n",
    "    return np.degrees(angle)\n",
    "\n",
    "def calculate_distance(point, line_x):\n",
    "    return abs(point[0] - line_x)\n",
    "\n",
    "def find_right_edge(contour):\n",
    "    epsilon = SMOOTHING_EPSILON_FACTOR * cv2.arcLength(contour, True)\n",
    "    approx_contour = cv2.approxPolyDP(contour, epsilon, True)\n",
    "    if len(approx_contour) > 4:\n",
    "        approx_contour = cv2.convexHull(approx_contour)\n",
    "        approx_contour = cv2.approxPolyDP(approx_contour, epsilon, True)\n",
    "    approx_contour = approx_contour[:4]\n",
    "    rightmost_points = sorted(approx_contour, key=lambda x: x[0][0])[-2:]\n",
    "    right_edge_start, right_edge_end = sorted(rightmost_points, key=lambda x: x[0][1])\n",
    "    return right_edge_start[0], right_edge_end[0]\n",
    "\n",
    "def average_edge_position(edge_history):\n",
    "    avg_start = np.mean([edge[0] for edge in edge_history], axis=0).astype(int)\n",
    "    avg_end = np.mean([edge[1] for edge in edge_history], axis=0).astype(int)\n",
    "    return tuple(avg_start), tuple(avg_end)\n",
    "\n",
    "def process_frame(frame, predictor, mask_percentage):\n",
    "    masked_frame, frame_mask = mask_sides(frame, mask_percentage)\n",
    "    outputs = predictor(masked_frame)\n",
    "    instances = outputs[\"instances\"].to(\"cpu\")\n",
    "    ramp_instances = instances[instances.pred_classes == 1]\n",
    "    draw_frame = masked_frame.copy()\n",
    "    for i in range(len(ramp_instances)):\n",
    "        if ramp_instances.has(\"pred_masks\"):\n",
    "            mask = ramp_instances.pred_masks[i].numpy()\n",
    "            mask = cv2.bitwise_and(mask.astype(np.uint8), frame_mask)\n",
    "            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            if contours:\n",
    "                largest_contour = max(contours, key=cv2.contourArea)\n",
    "                right_edge_start, right_edge_end = find_right_edge(largest_contour)\n",
    "                edge_history.append((right_edge_start, right_edge_end))\n",
    "                if len(edge_history) == FRAME_HISTORY:\n",
    "                    right_edge_start, right_edge_end = average_edge_position(edge_history)\n",
    "                cv2.line(draw_frame, right_edge_start, right_edge_end, EDGE_COLOR, LINE_THICKNESS)\n",
    "                \n",
    "                mid_x = draw_frame.shape[1] // 2\n",
    "                frame_height = draw_frame.shape[0]\n",
    "                green_line_start = int(frame_height * 0.28)  # Starting point of the green line (28% from the top)\n",
    "                green_line_end = frame_height  # Ending point of the green line (bottom of the frame)\n",
    "                cv2.line(draw_frame, (mid_x, green_line_start), (mid_x, green_line_end), GREEN_COLOR, LINE_THICKNESS)\n",
    "                \n",
    "                vector_right_edge = np.array([right_edge_end[0] - right_edge_start[0], right_edge_end[1] - right_edge_start[1]])\n",
    "                vector_green_line = np.array([0, -1])\n",
    "                angle_degrees = calculate_angle(vector_right_edge, vector_green_line)\n",
    "                distances = [calculate_distance(point, mid_x) for point in [right_edge_start, right_edge_end]]\n",
    "                min_distance = min(distances)\n",
    "                if angle_degrees < 5:\n",
    "                    if min_distance < 5:\n",
    "                        draw_frame = tint_screen(draw_frame, 'green', TRANSPARENCY_ALPHA)\n",
    "                    elif min_distance < 15:\n",
    "                        draw_frame = tint_screen(draw_frame, 'orange', TRANSPARENCY_ALPHA)\n",
    "                    else:\n",
    "                        draw_frame = tint_screen(draw_frame, 'red', TRANSPARENCY_ALPHA)\n",
    "                elif angle_degrees < 15:\n",
    "                    if min_distance < 15:\n",
    "                        draw_frame = tint_screen(draw_frame, 'orange', TRANSPARENCY_ALPHA)\n",
    "                    else:\n",
    "                        draw_frame = tint_screen(draw_frame, 'red', TRANSPARENCY_ALPHA)\n",
    "                else:\n",
    "                    draw_frame = tint_screen(draw_frame, 'red', TRANSPARENCY_ALPHA)\n",
    "    return draw_frame\n",
    "\n",
    "def process_video(video_path, predictor):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter('thisisoutput005.mp4', fourcc, fps, (width, height))\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        processed_frame = process_frame(frame, predictor, MASK_PERCENTAGE)\n",
    "        out.write(processed_frame)\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "# Example usage\n",
    "config_file = \"C:/Users/Daanish Mittal/OneDrive/Desktop/Tank_align/tank_alignment/army_ramp/loader.ramp/mask_rcnn_R_101_FPN_3x/2024-07-09-01-55-42/config.yaml\"\n",
    "weights_file = \"C:/Users/Daanish Mittal/OneDrive/Desktop/Tank_align/tank_alignment/army_ramp/loader.ramp/mask_rcnn_R_101_FPN_3x/2024-07-09-01-55-42/model_final.pth\"\n",
    "video_path = \"C:/Users/Daanish Mittal/OneDrive/Desktop/Tank_align/tank_alignment/test_army/WhatsApp Video 2024-07-18 at 15.22.47_a6e64e0a.mp4\"\n",
    "\n",
    "predictor = load_model(config_file, weights_file)\n",
    "process_video(video_path, predictor)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Model loaded successfully.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 151\u001B[0m\n\u001B[0;32m    148\u001B[0m weights_file \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mC:/Users/Daanish Mittal/OneDrive/Desktop/Tank_align/tank_alignment/army_ramp/loader.ramp/mask_rcnn_R_101_FPN_3x/2024-07-09-01-55-42/model_final.pth\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    149\u001B[0m video_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mC:/Users/Daanish Mittal/OneDrive/Desktop/Tank_align/tank_alignment/test_army/WhatsApp Video 2024-07-18 at 15.22.47_a6e64e0a.mp4\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 151\u001B[0m predictor \u001B[38;5;241m=\u001B[39m \u001B[43mload_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweights_file\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    152\u001B[0m process_video(video_path, predictor)\n",
      "Cell \u001B[1;32mIn[4], line 32\u001B[0m, in \u001B[0;36mload_model\u001B[1;34m(config_file, weights_file)\u001B[0m\n\u001B[0;32m     30\u001B[0m cfg\u001B[38;5;241m.\u001B[39mMODEL\u001B[38;5;241m.\u001B[39mROI_HEADS\u001B[38;5;241m.\u001B[39mSCORE_THRESH_TEST \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.97\u001B[39m  \u001B[38;5;66;03m# Set a custom testing threshold\u001B[39;00m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel loaded successfully.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 32\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mDefaultPredictor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcfg\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\detectron2\\engine\\defaults.py:288\u001B[0m, in \u001B[0;36mDefaultPredictor.__init__\u001B[1;34m(self, cfg)\u001B[0m\n\u001B[0;32m    285\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetadata \u001B[38;5;241m=\u001B[39m MetadataCatalog\u001B[38;5;241m.\u001B[39mget(cfg\u001B[38;5;241m.\u001B[39mDATASETS\u001B[38;5;241m.\u001B[39mTEST[\u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m    287\u001B[0m checkpointer \u001B[38;5;241m=\u001B[39m DetectionCheckpointer(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel)\n\u001B[1;32m--> 288\u001B[0m \u001B[43mcheckpointer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcfg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mMODEL\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mWEIGHTS\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    290\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maug \u001B[38;5;241m=\u001B[39m T\u001B[38;5;241m.\u001B[39mResizeShortestEdge(\n\u001B[0;32m    291\u001B[0m     [cfg\u001B[38;5;241m.\u001B[39mINPUT\u001B[38;5;241m.\u001B[39mMIN_SIZE_TEST, cfg\u001B[38;5;241m.\u001B[39mINPUT\u001B[38;5;241m.\u001B[39mMIN_SIZE_TEST], cfg\u001B[38;5;241m.\u001B[39mINPUT\u001B[38;5;241m.\u001B[39mMAX_SIZE_TEST\n\u001B[0;32m    292\u001B[0m )\n\u001B[0;32m    294\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_format \u001B[38;5;241m=\u001B[39m cfg\u001B[38;5;241m.\u001B[39mINPUT\u001B[38;5;241m.\u001B[39mFORMAT\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\detectron2\\checkpoint\\detection_checkpoint.py:62\u001B[0m, in \u001B[0;36mDetectionCheckpointer.load\u001B[1;34m(self, path, *args, **kwargs)\u001B[0m\n\u001B[0;32m     60\u001B[0m     path \u001B[38;5;241m=\u001B[39m parsed_url\u001B[38;5;241m.\u001B[39m_replace(query\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mgeturl()  \u001B[38;5;66;03m# remove query from filename\u001B[39;00m\n\u001B[0;32m     61\u001B[0m     path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpath_manager\u001B[38;5;241m.\u001B[39mget_local_path(path)\n\u001B[1;32m---> 62\u001B[0m ret \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mload(path, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m need_sync:\n\u001B[0;32m     65\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBroadcasting model states from main worker ...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\fvcore\\common\\checkpoint.py:155\u001B[0m, in \u001B[0;36mCheckpointer.load\u001B[1;34m(self, path, checkpointables)\u001B[0m\n\u001B[0;32m    152\u001B[0m     path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpath_manager\u001B[38;5;241m.\u001B[39mget_local_path(path)\n\u001B[0;32m    153\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misfile(path), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCheckpoint \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m not found!\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(path)\n\u001B[1;32m--> 155\u001B[0m checkpoint \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_load_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    156\u001B[0m incompatible \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_load_model(checkpoint)\n\u001B[0;32m    157\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    158\u001B[0m     incompatible \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    159\u001B[0m ):  \u001B[38;5;66;03m# handle some existing subclasses that returns None\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\detectron2\\checkpoint\\detection_checkpoint.py:99\u001B[0m, in \u001B[0;36mDetectionCheckpointer._load_file\u001B[1;34m(self, filename)\u001B[0m\n\u001B[0;32m     92\u001B[0m     model_state \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m     93\u001B[0m         k: v\n\u001B[0;32m     94\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_state\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mitems()\n\u001B[0;32m     95\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m k\u001B[38;5;241m.\u001B[39mendswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnum_batches_tracked\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     96\u001B[0m     }\n\u001B[0;32m     97\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m: model_state, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__author__\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpycls\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmatching_heuristics\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;01mTrue\u001B[39;00m}\n\u001B[1;32m---> 99\u001B[0m loaded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_torch_load\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m loaded:\n\u001B[0;32m    101\u001B[0m     loaded \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m: loaded}\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\detectron2\\checkpoint\\detection_checkpoint.py:114\u001B[0m, in \u001B[0;36mDetectionCheckpointer._torch_load\u001B[1;34m(self, f)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_torch_load\u001B[39m(\u001B[38;5;28mself\u001B[39m, f):\n\u001B[1;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_load_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\fvcore\\common\\checkpoint.py:252\u001B[0m, in \u001B[0;36mCheckpointer._load_file\u001B[1;34m(self, f)\u001B[0m\n\u001B[0;32m    240\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_load_file\u001B[39m(\u001B[38;5;28mself\u001B[39m, f: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Dict[\u001B[38;5;28mstr\u001B[39m, Any]:\n\u001B[0;32m    241\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    242\u001B[0m \u001B[38;5;124;03m    Load a checkpoint file. Can be overwritten by subclasses to support\u001B[39;00m\n\u001B[0;32m    243\u001B[0m \u001B[38;5;124;03m    different formats.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    250\u001B[0m \u001B[38;5;124;03m            to torch.Tensor or numpy arrays.\u001B[39;00m\n\u001B[0;32m    251\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 252\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcpu\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py:809\u001B[0m, in \u001B[0;36mload\u001B[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001B[0m\n\u001B[0;32m    807\u001B[0m             \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    808\u001B[0m                 \u001B[38;5;28;01mraise\u001B[39;00m pickle\u001B[38;5;241m.\u001B[39mUnpicklingError(UNSAFE_MESSAGE \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(e)) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m--> 809\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m _load(opened_zipfile, map_location, pickle_module, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpickle_load_args)\n\u001B[0;32m    810\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m weights_only:\n\u001B[0;32m    811\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py:1172\u001B[0m, in \u001B[0;36m_load\u001B[1;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001B[0m\n\u001B[0;32m   1170\u001B[0m unpickler \u001B[38;5;241m=\u001B[39m UnpicklerWrapper(data_file, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpickle_load_args)\n\u001B[0;32m   1171\u001B[0m unpickler\u001B[38;5;241m.\u001B[39mpersistent_load \u001B[38;5;241m=\u001B[39m persistent_load\n\u001B[1;32m-> 1172\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43munpickler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1174\u001B[0m torch\u001B[38;5;241m.\u001B[39m_utils\u001B[38;5;241m.\u001B[39m_validate_loaded_sparse_tensors()\n\u001B[0;32m   1176\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py:1142\u001B[0m, in \u001B[0;36m_load.<locals>.persistent_load\u001B[1;34m(saved_id)\u001B[0m\n\u001B[0;32m   1140\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1141\u001B[0m     nbytes \u001B[38;5;241m=\u001B[39m numel \u001B[38;5;241m*\u001B[39m torch\u001B[38;5;241m.\u001B[39m_utils\u001B[38;5;241m.\u001B[39m_element_size(dtype)\n\u001B[1;32m-> 1142\u001B[0m     typed_storage \u001B[38;5;241m=\u001B[39m \u001B[43mload_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnbytes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_maybe_decode_ascii\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlocation\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1144\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m typed_storage\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py:1112\u001B[0m, in \u001B[0;36m_load.<locals>.load_tensor\u001B[1;34m(dtype, numel, key, location)\u001B[0m\n\u001B[0;32m   1109\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_tensor\u001B[39m(dtype, numel, key, location):\n\u001B[0;32m   1110\u001B[0m     name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m-> 1112\u001B[0m     storage \u001B[38;5;241m=\u001B[39m \u001B[43mzip_file\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_storage_from_record\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnumel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mUntypedStorage\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39m_typed_storage()\u001B[38;5;241m.\u001B[39m_untyped_storage\n\u001B[0;32m   1113\u001B[0m     \u001B[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001B[39;00m\n\u001B[0;32m   1114\u001B[0m     \u001B[38;5;66;03m# stop wrapping with TypedStorage\u001B[39;00m\n\u001B[0;32m   1115\u001B[0m     typed_storage \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mstorage\u001B[38;5;241m.\u001B[39mTypedStorage(\n\u001B[0;32m   1116\u001B[0m         wrap_storage\u001B[38;5;241m=\u001B[39mrestore_location(storage, location),\n\u001B[0;32m   1117\u001B[0m         dtype\u001B[38;5;241m=\u001B[39mdtype,\n\u001B[0;32m   1118\u001B[0m         _internal\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "torch.cuda.set_device(0)  # Use the first GPU"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-24T07:47:33.711586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from collections import deque\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "import numpy as np\n",
    "\n",
    "# Constants\n",
    "MASK_PERCENTAGE = 0.2\n",
    "TRANSPARENCY_ALPHA = 0.3\n",
    "GREEN_COLOR = (0, 255, 0)\n",
    "ORANGE_COLOR = (0, 165, 255)\n",
    "RED_COLOR = (0, 0, 255)\n",
    "YELLOW_COLOR = (0, 255, 255)  # Changed from violet to yellow\n",
    "LINE_THICKNESS = 2\n",
    "SMOOTHING_EPSILON_FACTOR = 0.05\n",
    "FRAME_HISTORY = 5\n",
    "DESIRED_FPS = 5\n",
    "\n",
    "# Deque to store edge positions\n",
    "edge_history = deque(maxlen=FRAME_HISTORY)\n",
    "\n",
    "def load_model(config_file, weights_file):\n",
    "    print(\"Loading model...\")\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(config_file)\n",
    "    cfg.MODEL.WEIGHTS = weights_file\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.97  # Set a custom testing threshold\n",
    "    print(\"Model loaded successfully.\")\n",
    "    return DefaultPredictor(cfg)\n",
    "\n",
    "def mask_sides(image, mask_percentage):\n",
    "    h, w = image.shape[:2]\n",
    "    mask_width = int(w * mask_percentage)\n",
    "    mask = np.ones(image.shape[:2], dtype=np.uint8)\n",
    "    mask[:, :mask_width] = 0\n",
    "    mask[:, -mask_width:] = 0\n",
    "    masked_image = cv2.bitwise_and(image, image, mask=mask)\n",
    "    return masked_image, mask\n",
    "\n",
    "def tint_screen(image, color, alpha):\n",
    "    overlay = np.zeros_like(image)\n",
    "    if color == 'green':\n",
    "        overlay[:] = GREEN_COLOR\n",
    "    elif color == 'orange':\n",
    "        overlay[:] = ORANGE_COLOR\n",
    "    else:\n",
    "        overlay[:] = RED_COLOR\n",
    "    cv2.addWeighted(overlay, alpha, image, 1 - alpha, 0, image)\n",
    "    return image\n",
    "\n",
    "def calculate_angle(vector1, vector2):\n",
    "    if vector1[1] > 0:\n",
    "        vector1 = -vector1\n",
    "    if vector2[1] > 0:\n",
    "        vector2 = -vector2\n",
    "    unit_vector1 = vector1 / np.linalg.norm(vector1)\n",
    "    unit_vector2 = vector2 / np.linalg.norm(vector2)\n",
    "    dot_product = np.dot(unit_vector1, unit_vector2)\n",
    "    angle = np.arccos(np.clip(dot_product, -1.0, 1.0))\n",
    "    return np.degrees(angle)\n",
    "\n",
    "def calculate_distance(point, line_x):\n",
    "    return abs(point[0] - line_x)\n",
    "\n",
    "def find_right_edge(contour):\n",
    "    epsilon = SMOOTHING_EPSILON_FACTOR * cv2.arcLength(contour, True)\n",
    "    approx_contour = cv2.approxPolyDP(contour, epsilon, True)\n",
    "    if len(approx_contour) > 4:\n",
    "        approx_contour = cv2.convexHull(approx_contour)\n",
    "        approx_contour = cv2.approxPolyDP(approx_contour, epsilon, True)\n",
    "    approx_contour = approx_contour[:4]\n",
    "    rightmost_points = sorted(approx_contour, key=lambda x: x[0][0])[-2:]\n",
    "    right_edge_start, right_edge_end = sorted(rightmost_points, key=lambda x: x[0][1])\n",
    "    return right_edge_start[0], right_edge_end[0]\n",
    "\n",
    "def average_edge_position(edge_history):\n",
    "    avg_start = np.mean([edge[0] for edge in edge_history], axis=0).astype(int)\n",
    "    avg_end = np.mean([edge[1] for edge in edge_history], axis=0).astype(int)\n",
    "    return tuple(avg_start), tuple(avg_end)\n",
    "\n",
    "def process_frame(frame, predictor, mask_percentage):\n",
    "    masked_frame, frame_mask = mask_sides(frame, mask_percentage)\n",
    "    outputs = predictor(masked_frame)\n",
    "    instances = outputs[\"instances\"].to(\"cpu\")\n",
    "    ramp_instances = instances[instances.pred_classes == 1]\n",
    "    draw_frame = masked_frame.copy()\n",
    "    for i in range(len(ramp_instances)):\n",
    "        if ramp_instances.has(\"pred_masks\"):\n",
    "            mask = ramp_instances.pred_masks[i].numpy()\n",
    "            mask = cv2.bitwise_and(mask.astype(np.uint8), frame_mask)\n",
    "            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            if contours:\n",
    "                largest_contour = max(contours, key=cv2.contourArea)\n",
    "                \n",
    "                # Highlight the detected object\n",
    "                cv2.drawContours(draw_frame, [largest_contour], 0, YELLOW_COLOR, 2)\n",
    "                \n",
    "                right_edge_start, right_edge_end = find_right_edge(largest_contour)\n",
    "                edge_history.append((right_edge_start, right_edge_end))\n",
    "                if len(edge_history) == FRAME_HISTORY:\n",
    "                    right_edge_start, right_edge_end = average_edge_position(edge_history)\n",
    "                cv2.line(draw_frame, right_edge_start, right_edge_end, YELLOW_COLOR, LINE_THICKNESS)\n",
    "                \n",
    "                mid_x = draw_frame.shape[1] // 2\n",
    "                frame_height = draw_frame.shape[0]\n",
    "                green_line_start = int(frame_height * 0.28)  # Starting point of the green line (28% from the top)\n",
    "                green_line_end = frame_height  # Ending point of the green line (bottom of the frame)\n",
    "                cv2.line(draw_frame, (mid_x, green_line_start), (mid_x, green_line_end), GREEN_COLOR, LINE_THICKNESS)\n",
    "                \n",
    "                vector_right_edge = np.array([right_edge_end[0] - right_edge_start[0], right_edge_end[1] - right_edge_start[1]])\n",
    "                vector_green_line = np.array([0, -1])\n",
    "                angle_degrees = calculate_angle(vector_right_edge, vector_green_line)\n",
    "                distances = [calculate_distance(point, mid_x) for point in [right_edge_start, right_edge_end]]\n",
    "                min_distance = min(distances)\n",
    "                if angle_degrees < 5:\n",
    "                    if min_distance < 5:\n",
    "                        draw_frame = tint_screen(draw_frame, 'green', TRANSPARENCY_ALPHA)\n",
    "                    elif min_distance < 15:\n",
    "                        draw_frame = tint_screen(draw_frame, 'orange', TRANSPARENCY_ALPHA)\n",
    "                    else:\n",
    "                        draw_frame = tint_screen(draw_frame, 'red', TRANSPARENCY_ALPHA)\n",
    "                elif angle_degrees < 15:\n",
    "                    if min_distance < 15:\n",
    "                        draw_frame = tint_screen(draw_frame, 'orange', TRANSPARENCY_ALPHA)\n",
    "                    else:\n",
    "                        draw_frame = tint_screen(draw_frame, 'red', TRANSPARENCY_ALPHA)\n",
    "                else:\n",
    "                    draw_frame = tint_screen(draw_frame, 'red', TRANSPARENCY_ALPHA)\n",
    "    return draw_frame\n",
    "\n",
    "\n",
    "def process_video(video_path, predictor):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    original_fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    frame_interval = max(1, original_fps // DESIRED_FPS)\n",
    "    frame_count = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Skip frames to achieve the desired fps\n",
    "        if frame_count % frame_interval == 0:\n",
    "            processed_frame = process_frame(frame, predictor, MASK_PERCENTAGE)\n",
    "            \n",
    "            # Resize frame to 70% of original size\n",
    "            resized_frame = cv2.resize(processed_frame, (int(width * 0.7), int(height * 0.7)))\n",
    "            \n",
    "            # Display the resulting frame\n",
    "            cv2.imshow('Processed Video', resized_frame)\n",
    "            \n",
    "            # Press 'q' to quit\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        \n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "config_file = \"C:/Users/Daanish Mittal/OneDrive/Desktop/Tank_align/tank_alignment/army_ramp/loader.ramp/mask_rcnn_R_101_FPN_3x/2024-07-09-01-55-42/config.yaml\"\n",
    "weights_file = \"C:/Users/Daanish Mittal/OneDrive/Desktop/Tank_align/tank_alignment/army_ramp/loader.ramp/mask_rcnn_R_101_FPN_3x/2024-07-09-01-55-42/model_final.pth\"\n",
    "video_path = \"C:/Users/Daanish Mittal/OneDrive/Desktop/Tank_align/tank_alignment/test_army/WhatsApp Video 2024-07-18 at 15.22.47_a6e64e0a.mp4\"\n",
    "\n",
    "predictor = load_model(config_file, weights_file)\n",
    "process_video(video_path, predictor)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Model loaded successfully.\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "****ramp with ipcam****"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
