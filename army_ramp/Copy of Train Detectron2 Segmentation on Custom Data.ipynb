{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZBUwM3tyFWS"
   },
   "source": [
    "Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator` and set it to `GPU`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T20:25:12.873654Z",
     "start_time": "2024-07-08T20:25:12.711306Z"
    }
   },
   "source": "!nvidia-smi\n",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul  9 01:55:12 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 556.12                 Driver Version: 556.12         CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   69C    P0             26W /   88W |       0MiB /   6144MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6MJ8SshpLaU3"
   },
   "source": [
    "## Install Detectron2 and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T08:00:27.100950Z",
     "start_time": "2024-06-25T07:59:43.324225Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 51415,
     "status": "ok",
     "timestamp": 1719261763571,
     "user": {
      "displayName": "Daanish Mittal",
      "userId": "09086078631529623014"
     },
     "user_tz": -330
    },
    "id": "fM1JmUCQLdKp",
    "outputId": "cb684e1a-65eb-4798-eb32-28ef5b785883"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/facebookresearch/detectron2.git\n",
      "  Cloning https://github.com/facebookresearch/detectron2.git to c:\\users\\daanish mittal\\appdata\\local\\temp\\pip-req-build-3t47tbqr\n",
      "  Resolved https://github.com/facebookresearch/detectron2.git to commit 70f454304e1a38378200459dd2dbca0f0f4a5ab4\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: Pillow>=7.1 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from detectron2==0.6) (10.2.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from detectron2==0.6) (3.7.0)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from detectron2==0.6) (2.0.8)\n",
      "Requirement already satisfied: termcolor>=1.1 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from detectron2==0.6) (2.3.0)\n",
      "Requirement already satisfied: yacs>=0.1.8 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from detectron2==0.6) (0.1.8)\n",
      "Requirement already satisfied: tabulate in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from detectron2==0.6) (0.8.10)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from detectron2==0.6) (2.0.0)\n",
      "Requirement already satisfied: tqdm>4.29.0 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from detectron2==0.6) (4.64.1)\n",
      "Requirement already satisfied: tensorboard in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from detectron2==0.6) (2.10.1)\n",
      "Requirement already satisfied: fvcore<0.1.6,>=0.1.5 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from detectron2==0.6) (0.1.5.post20221221)\n",
      "Requirement already satisfied: iopath<0.1.10,>=0.1.7 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from detectron2==0.6) (0.1.9)\n",
      "Requirement already satisfied: omegaconf<2.4,>=2.1 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from detectron2==0.6) (2.3.0)\n",
      "Requirement already satisfied: hydra-core>=1.1 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from detectron2==0.6) (1.3.2)\n",
      "Requirement already satisfied: black in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from detectron2==0.6) (22.6.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from detectron2==0.6) (22.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.26.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from hydra-core>=1.1->detectron2==0.6) (4.9.3)\n",
      "Requirement already satisfied: portalocker in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from iopath<0.1.10,>=0.1.7->detectron2==0.6) (2.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from matplotlib->detectron2==0.6) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from matplotlib->detectron2==0.6) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from matplotlib->detectron2==0.6) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from matplotlib->detectron2==0.6) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from matplotlib->detectron2==0.6) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from matplotlib->detectron2==0.6) (2.8.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from tqdm>4.29.0->detectron2==0.6) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from black->detectron2==0.6) (8.0.4)\n",
      "Requirement already satisfied: platformdirs>=2 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from black->detectron2==0.6) (2.5.2)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from black->detectron2==0.6) (0.10.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from black->detectron2==0.6) (0.4.3)\n",
      "Requirement already satisfied: tomli>=1.1.0 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from black->detectron2==0.6) (2.0.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from tensorboard->detectron2==0.6) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from tensorboard->detectron2==0.6) (1.57.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from tensorboard->detectron2==0.6) (2.22.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from tensorboard->detectron2==0.6) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from tensorboard->detectron2==0.6) (3.4.1)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from tensorboard->detectron2==0.6) (3.19.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from tensorboard->detectron2==0.6) (2.28.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from tensorboard->detectron2==0.6) (65.6.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from tensorboard->detectron2==0.6) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from tensorboard->detectron2==0.6) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from tensorboard->detectron2==0.6) (2.2.2)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from tensorboard->detectron2==0.6) (0.38.4)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (4.9)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (1.16.0)\n",
      "Requirement already satisfied: urllib3<2.0 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (1.26.13)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2==0.6) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (2.1.5)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from portalocker->iopath<0.1.10,>=0.1.7->detectron2==0.6) (305.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2==0.6) (3.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git 'C:\\Users\\Daanish Mittal\\AppData\\Local\\Temp\\pip-req-build-3t47tbqr'\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/facebookresearch/detectron2.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_V8w1ew59buh"
   },
   "source": [
    "Now is a good time to confirm that we have the right versions of the libraries at our disposal."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 4612,
     "status": "error",
     "timestamp": 1719261915922,
     "user": {
      "displayName": "Daanish Mittal",
      "userId": "09086078631529623014"
     },
     "user_tz": -330
    },
    "id": "sqCNglJXRro5",
    "outputId": "3f148ff1-b579-4dfa-c1f0-adb5bdda4c98",
    "ExecuteTime": {
     "end_time": "2024-07-08T20:25:25.061607Z",
     "start_time": "2024-07-08T20:25:16.684809Z"
    }
   },
   "source": [
    "%pip install -U \"iopath<0.1.10,>=0.1.7\"\n",
    "import torch, detectron2\n",
    "!nvcc --version\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
    "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
    "import pkg_resources\n",
    "print(\"detectron2:\", pkg_resources.get_distribution(\"detectron2\").version)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: iopath<0.1.10,>=0.1.7 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (0.1.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from iopath<0.1.10,>=0.1.7) (4.64.1)\n",
      "Requirement already satisfied: portalocker in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from iopath<0.1.10,>=0.1.7) (2.8.2)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from portalocker->iopath<0.1.10,>=0.1.7) (305.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from tqdm->iopath<0.1.10,>=0.1.7) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2019 NVIDIA Corporation\n",
      "Built on Fri_Feb__8_19:08:26_Pacific_Standard_Time_2019\n",
      "Cuda compilation tools, release 10.1, V10.1.105\n",
      "torch:  2.0 ; cuda:  cu117\n",
      "detectron2: 0.6\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "executionInfo": {
     "elapsed": 154,
     "status": "error",
     "timestamp": 1719261789459,
     "user": {
      "displayName": "Daanish Mittal",
      "userId": "09086078631529623014"
     },
     "user_tz": -330
    },
    "id": "DIEKfPKFmW54",
    "outputId": "1eae8bc5-084a-4dd2-bcdd-00efdcfc6330",
    "ExecuteTime": {
     "end_time": "2024-07-08T20:25:26.449655Z",
     "start_time": "2024-07-08T20:25:25.061607Z"
    }
   },
   "source": [
    "# COMMON LIBRARIES\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "from datetime import datetime\n",
    "import cv2\n",
    "\n",
    "# DATA SET PREPARATION AND LOADING\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "\n",
    "# VISUALIZATION\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "\n",
    "# CONFIGURATION\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "\n",
    "# EVALUATION\n",
    "from detectron2.engine import DefaultPredictor\n",
    "\n",
    "# TRAINING\n",
    "from detectron2.engine import DefaultTrainer"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LOszeLVlErvk"
   },
   "source": [
    "## Run a Pre-trained Detectron2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T08:01:49.176003Z",
     "start_time": "2024-06-25T08:00:40.278498Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "executionInfo": {
     "elapsed": 2681,
     "status": "ok",
     "timestamp": 1668002463033,
     "user": {
      "displayName": "Piotr Skalski",
      "userId": "04309230031174164349"
     },
     "user_tz": -60
    },
    "id": "T8sfLDV7FTYD",
    "outputId": "0e8eea7a-6407-4cb5-a7b8-6f4997bacde0"
   },
   "outputs": [],
   "source": [
    "# !wget http://images.cocodataset.org/val2017/000000439715.jpg -q -O input.jpg\n",
    "# image = cv2.imread(\"./input.jpg\")\n",
    "# cv2.imshow('Display Window', image)  # 'Display Window' is the name of the window\n",
    "# cv2.waitKey(0)  # Waits for a key press to close the window\n",
    "# cv2.destroyAllWindows()  # Closes the window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T08:02:05.116019Z",
     "start_time": "2024-06-25T08:01:49.193280Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu117\n",
      "Collecting torch\n",
      "  Using cached https://download.pytorch.org/whl/cu117/torch-2.0.1%2Bcu117-cp310-cp310-win_amd64.whl (2343.6 MB)\n",
      "Collecting torchvision\n",
      "  Using cached https://download.pytorch.org/whl/cu117/torchvision-0.15.2%2Bcu117-cp310-cp310-win_amd64.whl (4.9 MB)\n",
      "Collecting torchaudio\n",
      "  Using cached https://download.pytorch.org/whl/cu117/torchaudio-2.0.2%2Bcu117-cp310-cp310-win_amd64.whl (2.5 MB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Collecting typing-extensions (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
      "Collecting sympy (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "Collecting numpy (from torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/numpy-1.26.3-cp310-cp310-win_amd64.whl (15.8 MB)\n",
      "Collecting requests (from torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/requests-2.28.1-py3-none-any.whl (62 kB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/pillow-10.2.0-cp310-cp310-win_amd64.whl (2.6 MB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached https://download.pytorch.org/whl/MarkupSafe-2.1.5-cp310-cp310-win_amd64.whl (17 kB)\n",
      "Collecting charset-normalizer<3,>=2 (from requests->torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1 (from requests->torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
      "Collecting mpmath>=0.19 (from sympy->torch)\n",
      "  Using cached https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, urllib3, typing-extensions, sympy, pillow, numpy, networkx, MarkupSafe, idna, filelock, charset-normalizer, certifi, requests, jinja2, torch, torchvision, torchaudio\n",
      "  Attempting uninstall: mpmath\n",
      "    Found existing installation: mpmath 1.3.0\n",
      "    Uninstalling mpmath-1.3.0:\n",
      "      Successfully uninstalled mpmath-1.3.0\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.13\n",
      "    Uninstalling urllib3-1.26.13:\n",
      "      Successfully uninstalled urllib3-1.26.13\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.9.0\n",
      "    Uninstalling typing_extensions-4.9.0:\n",
      "      Successfully uninstalled typing_extensions-4.9.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.12\n",
      "    Uninstalling sympy-1.12:\n",
      "      Successfully uninstalled sympy-1.12\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: pillow 10.2.0\n",
      "    Uninstalling pillow-10.2.0:\n",
      "      Successfully uninstalled pillow-10.2.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.3\n",
      "    Uninstalling numpy-1.26.3:\n",
      "      Successfully uninstalled numpy-1.26.3\n",
      "  Attempting uninstall: networkx\n",
      "    Found existing installation: networkx 3.2.1\n",
      "    Uninstalling networkx-3.2.1:\n",
      "      Successfully uninstalled networkx-3.2.1\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 2.1.5\n",
      "    Uninstalling MarkupSafe-2.1.5:\n",
      "      Successfully uninstalled MarkupSafe-2.1.5\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.4\n",
      "    Uninstalling idna-3.4:\n",
      "      Successfully uninstalled idna-3.4\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.13.1\n",
      "    Uninstalling filelock-3.13.1:\n",
      "      Successfully uninstalled filelock-3.13.1\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 2.1.1\n",
      "    Uninstalling charset-normalizer-2.1.1:\n",
      "      Successfully uninstalled charset-normalizer-2.1.1\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2022.12.7\n",
      "    Uninstalling certifi-2022.12.7:\n",
      "      Successfully uninstalled certifi-2022.12.7\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.28.1\n",
      "    Uninstalling requests-2.28.1:\n",
      "      Successfully uninstalled requests-2.28.1\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 3.1.3\n",
      "    Uninstalling Jinja2-3.1.3:\n",
      "      Successfully uninstalled Jinja2-3.1.3\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.0.1+cu117\n",
      "    Uninstalling torch-2.0.1+cu117:\n",
      "      Successfully uninstalled torch-2.0.1+cu117\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.15.2+cu117\n",
      "    Uninstalling torchvision-0.15.2+cu117:\n",
      "      Successfully uninstalled torchvision-0.15.2+cu117\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.0.2+cu117\n",
      "    Uninstalling torchaudio-2.0.2+cu117:\n",
      "      Successfully uninstalled torchaudio-2.0.2+cu117\n",
      "Successfully installed MarkupSafe-2.1.5 certifi-2022.12.7 charset-normalizer-2.1.1 filelock-3.13.1 idna-3.4 jinja2-3.1.3 mpmath-1.3.0 networkx-3.2.1 numpy-1.26.3 pillow-10.2.0 requests-2.28.1 sympy-1.12 torch-2.0.1+cu117 torchaudio-2.0.2+cu117 torchvision-0.15.2+cu117 typing-extensions-4.9.0 urllib3-1.26.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Daanish Mittal\\anaconda3\\Lib\\site-packages\\~2l'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Daanish Mittal\\anaconda3\\Lib\\site-packages\\~1mpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Daanish Mittal\\anaconda3\\Lib\\site-packages\\~2mpy'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Daanish Mittal\\anaconda3\\Lib\\site-packages\\~2rkupsafe'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Daanish Mittal\\anaconda3\\Lib\\site-packages\\~1fuser'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Daanish Mittal\\anaconda3\\Lib\\site-packages\\~2rch'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Daanish Mittal\\anaconda3\\Lib\\site-packages\\~1rchvision'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.41 requires requests_mock, which is not installed.\n",
      "gensim 4.3.0 requires FuzzyTM>=0.4.0, which is not installed.\n",
      "conda-repo-cli 1.0.41 requires clyent==1.2.1, but you have clyent 1.2.2 which is incompatible.\n",
      "conda-repo-cli 1.0.41 requires nbformat==5.4.0, but you have nbformat 5.7.0 which is incompatible.\n",
      "mediapipe 0.10.14 requires protobuf<5,>=4.25.3, but you have protobuf 3.19.6 which is incompatible.\n",
      "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.26.3 which is incompatible.\n",
      "roboflow 1.1.33 requires idna==3.7, but you have idna 3.4 which is incompatible.\n",
      "tensorflow-datasets 4.9.2 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
      "tensorflow-intel 2.13.0 requires keras<2.14,>=2.13.1, but you have keras 2.10.0 which is incompatible.\n",
      "tensorflow-intel 2.13.0 requires numpy<=1.24.3,>=1.22, but you have numpy 1.26.3 which is incompatible.\n",
      "tensorflow-intel 2.13.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\n",
      "tensorflow-intel 2.13.0 requires tensorboard<2.14,>=2.13, but you have tensorboard 2.10.1 which is incompatible.\n",
      "tensorflow-intel 2.13.0 requires tensorflow-estimator<2.14,>=2.13.0, but you have tensorflow-estimator 2.10.0 which is incompatible.\n",
      "tensorflow-intel 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.9.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117 --upgrade --force-reinstall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFkJOTWvxu6G"
   },
   "source": [
    "## COCO Format Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: roboflow in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (1.1.33)\n",
      "Requirement already satisfied: certifi in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from roboflow) (2022.12.7)\n",
      "Requirement already satisfied: chardet==4.0.0 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from roboflow) (4.0.0)\n",
      "Collecting idna==3.7 (from roboflow)\n",
      "  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: cycler in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from roboflow) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from roboflow) (1.4.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from roboflow) (3.7.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from roboflow) (1.26.3)\n",
      "Requirement already satisfied: opencv-python-headless==4.10.0.84 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from roboflow) (4.10.0.84)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from roboflow) (10.2.0)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from roboflow) (2.8.2)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from roboflow) (1.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from roboflow) (2.28.1)\n",
      "Requirement already satisfied: six in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from roboflow) (1.16.0)\n",
      "Requirement already satisfied: urllib3>=1.26.6 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from roboflow) (1.26.13)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from roboflow) (4.64.1)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from roboflow) (6.0)\n",
      "Requirement already satisfied: requests-toolbelt in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from roboflow) (0.9.1)\n",
      "Requirement already satisfied: python-magic in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from roboflow) (0.4.27)\n",
      "Requirement already satisfied: colorama in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from tqdm>=4.41.0->roboflow) (0.4.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from matplotlib->roboflow) (1.0.5)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from matplotlib->roboflow) (4.25.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from matplotlib->roboflow) (22.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from matplotlib->roboflow) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\daanish mittal\\anaconda3\\lib\\site-packages (from requests->roboflow) (2.1.1)\n",
      "Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Installing collected packages: idna\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.4\n",
      "    Uninstalling idna-3.4:\n",
      "      Successfully uninstalled idna-3.4\n",
      "Successfully installed idna-3.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-datasets 4.9.2 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
      "tensorflow-intel 2.13.0 requires keras<2.14,>=2.13.1, but you have keras 2.10.0 which is incompatible.\n",
      "tensorflow-intel 2.13.0 requires numpy<=1.24.3,>=1.22, but you have numpy 1.26.3 which is incompatible.\n",
      "tensorflow-intel 2.13.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\n",
      "tensorflow-intel 2.13.0 requires tensorboard<2.14,>=2.13, but you have tensorboard 2.10.1 which is incompatible.\n",
      "tensorflow-intel 2.13.0 requires tensorflow-estimator<2.14,>=2.13.0, but you have tensorflow-estimator 2.10.0 which is incompatible.\n",
      "tensorflow-intel 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.9.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install roboflow"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14044,
     "status": "ok",
     "timestamp": 1668003461405,
     "user": {
      "displayName": "Piotr Skalski",
      "userId": "04309230031174164349"
     },
     "user_tz": -60
    },
    "id": "grFIdy8ynP-7",
    "outputId": "d05be873-4ac3-4d10-d88b-fa053a263c90",
    "ExecuteTime": {
     "end_time": "2024-07-08T20:25:29.261621Z",
     "start_time": "2024-07-08T20:25:26.453490Z"
    }
   },
   "source": [
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"RovqaIFPpcekpUVWjRke\")\n",
    "project = rf.workspace(\"tank-5yib6\").project(\"loader.ramp\")\n",
    "version = project.version(2)\n",
    "dataset = version.download(\"coco-segmentation\")\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aoB31yi4AoYs"
   },
   "source": [
    "### Register"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HopUGOyW853G"
   },
   "source": [
    "When you use Detectron2, before you actually train the model you need to [register it](https://detectron2.readthedocs.io/en/latest/tutorials/datasets.html#register-a-coco-format-dataset)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KbI2PNEZF3sU",
    "ExecuteTime": {
     "end_time": "2024-07-08T20:25:29.271785Z",
     "start_time": "2024-07-08T20:25:29.261621Z"
    }
   },
   "source": [
    "DATA_SET_NAME = dataset.name.replace(\" \", \"-\")\n",
    "ANNOTATIONS_FILE_NAME = \"_annotations.coco.json\""
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jntOI8GJG2ks",
    "ExecuteTime": {
     "end_time": "2024-07-08T20:25:29.282065Z",
     "start_time": "2024-07-08T20:25:29.271785Z"
    }
   },
   "source": [
    "# TRAIN SET\n",
    "TRAIN_DATA_SET_NAME = f\"{DATA_SET_NAME}-train\"\n",
    "TRAIN_DATA_SET_IMAGES_DIR_PATH = os.path.join(dataset.location, \"train\")\n",
    "TRAIN_DATA_SET_ANN_FILE_PATH = os.path.join(dataset.location, \"train\", ANNOTATIONS_FILE_NAME)\n",
    "\n",
    "register_coco_instances(\n",
    "    name=TRAIN_DATA_SET_NAME,\n",
    "    metadata={},\n",
    "    json_file=TRAIN_DATA_SET_ANN_FILE_PATH,\n",
    "    image_root=TRAIN_DATA_SET_IMAGES_DIR_PATH\n",
    ")\n",
    "\n",
    "# TEST SET\n",
    "TEST_DATA_SET_NAME = f\"{DATA_SET_NAME}-test\"\n",
    "TEST_DATA_SET_IMAGES_DIR_PATH = os.path.join(dataset.location, \"test\")\n",
    "TEST_DATA_SET_ANN_FILE_PATH = os.path.join(dataset.location, \"test\", ANNOTATIONS_FILE_NAME)\n",
    "\n",
    "register_coco_instances(\n",
    "    name=TEST_DATA_SET_NAME,\n",
    "    metadata={},\n",
    "    json_file=TEST_DATA_SET_ANN_FILE_PATH,\n",
    "    image_root=TEST_DATA_SET_IMAGES_DIR_PATH\n",
    ")\n",
    "\n",
    "# VALID SET\n",
    "VALID_DATA_SET_NAME = f\"{DATA_SET_NAME}-valid\"\n",
    "VALID_DATA_SET_IMAGES_DIR_PATH = os.path.join(dataset.location, \"valid\")\n",
    "VALID_DATA_SET_ANN_FILE_PATH = os.path.join(dataset.location, \"valid\", ANNOTATIONS_FILE_NAME)\n",
    "\n",
    "register_coco_instances(\n",
    "    name=VALID_DATA_SET_NAME,\n",
    "    metadata={},\n",
    "    json_file=VALID_DATA_SET_ANN_FILE_PATH,\n",
    "    image_root=VALID_DATA_SET_IMAGES_DIR_PATH\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOCY1UWNCtnq"
   },
   "source": [
    "We can now confirm that our custom dataset was correctly registered using [MetadataCatalog](https://detectron2.readthedocs.io/en/latest/modules/data.html#detectron2.data.MetadataCatalog)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 439,
     "status": "ok",
     "timestamp": 1668003586610,
     "user": {
      "displayName": "Piotr Skalski",
      "userId": "04309230031174164349"
     },
     "user_tz": -60
    },
    "id": "LR8ha4EHCkA-",
    "outputId": "6506603a-3742-43ee-af5d-7f842426d25d",
    "ExecuteTime": {
     "end_time": "2024-07-08T20:25:29.292194Z",
     "start_time": "2024-07-08T20:25:29.282065Z"
    }
   },
   "source": [
    "[\n",
    "    data_set\n",
    "    for data_set\n",
    "    in MetadataCatalog.list()\n",
    "    if data_set.startswith(DATA_SET_NAME)\n",
    "]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loader.ramp-train', 'loader.ramp-test', 'loader.ramp-valid']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDpU2L3UL922"
   },
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4Bd_-oCA90a"
   },
   "source": [
    "Let's take a look at single entry from out train dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 709
    },
    "executionInfo": {
     "elapsed": 1256,
     "status": "ok",
     "timestamp": 1668003725711,
     "user": {
      "displayName": "Piotr Skalski",
      "userId": "04309230031174164349"
     },
     "user_tz": -60
    },
    "id": "eE0anblvMGJx",
    "outputId": "b5db94b3-faf0-4c64-9717-fa906c4b76db",
    "ExecuteTime": {
     "end_time": "2024-07-08T20:25:40.405247Z",
     "start_time": "2024-07-08T20:25:39.025195Z"
    }
   },
   "source": [
    "metadata = MetadataCatalog.get(TRAIN_DATA_SET_NAME)\n",
    "dataset_train = DatasetCatalog.get(TRAIN_DATA_SET_NAME)\n",
    "\n",
    "dataset_entry = dataset_train[0]\n",
    "image = cv2.imread(dataset_entry[\"file_name\"])\n",
    "\n",
    "visualizer = Visualizer(\n",
    "    image[:, :, ::-1],\n",
    "    metadata=metadata,\n",
    "    scale=0.6,\n",
    "    instance_mode=ColorMode.IMAGE_BW\n",
    ")\n",
    "\n",
    "out = visualizer.draw_dataset_dict(dataset_entry)\n",
    "cv2.imshow(\"visualize\",out.get_image()[:, :, ::-1])\n",
    "cv2.waitKey(0)  # Waits for a key press to close the window\n",
    "cv2.destroyAllWindows()  # Closes the window"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GavGRHy2M7Hb"
   },
   "source": [
    "## Train Model Using Custom COCO Format Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZ3g-l56NMOY"
   },
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "krCm2L_lNC83",
    "ExecuteTime": {
     "end_time": "2024-07-08T20:25:42.300329Z",
     "start_time": "2024-07-08T20:25:42.288495Z"
    }
   },
   "source": [
    "# HYPERPARAMETERS\n",
    "ARCHITECTURE = \"mask_rcnn_R_101_FPN_3x\"\n",
    "CONFIG_FILE_PATH = f\"COCO-InstanceSegmentation/{ARCHITECTURE}.yaml\"\n",
    "MAX_ITER = 3000\n",
    "EVAL_PERIOD = 400\n",
    "BASE_LR = 0.0001\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "# OUTPUT DIRa\n",
    "OUTPUT_DIR_PATH = os.path.join(\n",
    "    DATA_SET_NAME,\n",
    "    ARCHITECTURE,\n",
    "    datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    ")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR_PATH, exist_ok=True)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T20:25:46.006421Z",
     "start_time": "2024-07-08T20:25:45.965420Z"
    }
   },
   "source": [
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(CONFIG_FILE_PATH))\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(CONFIG_FILE_PATH)\n",
    "cfg.DATASETS.TRAIN = (TRAIN_DATA_SET_NAME,)\n",
    "cfg.DATASETS.TEST = (TEST_DATA_SET_NAME,)\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512 # Default batch size per image\n",
    "\n",
    "# Adjusting batch size and image size for GPU with 6GB VRAM\n",
    "cfg.SOLVER.IMS_PER_BATCH = 4  # Overall batch size\n",
    "cfg.INPUT.MIN_SIZE_TRAIN = (640, 672, 704, 736, 768, 800)  # Adjusted to fit memory constraints\n",
    "cfg.INPUT.MIN_SIZE_TEST = 800\n",
    "cfg.INPUT.MAX_SIZE_TRAIN = 1333\n",
    "cfg.INPUT.MAX_SIZE_TEST = 1333\n",
    "\n",
    "cfg.TEST.EVAL_PERIOD = EVAL_PERIOD\n",
    "cfg.DATALOADER.NUM_WORKERS = 4\n",
    "cfg.SOLVER.BASE_LR = BASE_LR\n",
    "cfg.SOLVER.MAX_ITER = MAX_ITER\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = NUM_CLASSES\n",
    "cfg.INPUT.MASK_FORMAT = 'bitmask'\n",
    "cfg.OUTPUT_DIR = OUTPUT_DIR_PATH\n",
    "\n",
    "# Save config for future reference\n",
    "with open(os.path.join(OUTPUT_DIR_PATH, \"config.yaml\"), \"w\") as f:\n",
    "    f.write(cfg.dump())"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ch-_5aCuXWj9"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 961595,
     "status": "ok",
     "timestamp": 1668005381143,
     "user": {
      "displayName": "Piotr Skalski",
      "userId": "04309230031174164349"
     },
     "user_tz": -60
    },
    "id": "7S8y2W2AQvJq",
    "outputId": "36fe2d26-1118-4748-fff1-18a86d873970",
    "ExecuteTime": {
     "end_time": "2024-07-08T21:20:05.603897Z",
     "start_time": "2024-07-08T20:25:48.640426Z"
    }
   },
   "source": [
    "trainer = DefaultTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[07/09 01:55:50 d2.engine.defaults]: \u001B[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (6): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (7): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (8): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (9): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (10): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (11): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (12): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (13): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (14): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (15): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (16): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (17): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (18): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (19): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (20): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (21): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (22): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=4, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=12, bias=True)\n",
      "    )\n",
      "    (mask_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (mask_head): MaskRCNNConvUpsampleHead(\n",
      "      (mask_fcn1): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn2): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn3): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn4): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (deconv_relu): ReLU()\n",
      "      (predictor): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/09 01:55:50 d2.data.datasets.coco]: \u001B[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001B[32m[07/09 01:55:50 d2.data.datasets.coco]: \u001B[0mLoaded 377 images in COCO format from C:\\Users\\Daanish Mittal\\OneDrive\\Desktop\\Tank_align\\tank_alignment\\army_ramp\\loader.ramp-2\\train\\_annotations.coco.json\n",
      "\u001B[32m[07/09 01:55:50 d2.data.build]: \u001B[0mRemoved 3 images with no usable annotations. 374 images left.\n",
      "\u001B[32m[07/09 01:55:50 d2.data.build]: \u001B[0mDistribution of instances among all 2 categories:\n",
      "\u001B[36m|  category   | #instances   |  category  | #instances   |\n",
      "|:-----------:|:-------------|:----------:|:-------------|\n",
      "| loader-ramp | 0            |    ramp    | 659          |\n",
      "|             |              |            |              |\n",
      "|    total    | 659          |            |              |\u001B[0m\n",
      "\u001B[32m[07/09 01:55:50 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "\u001B[32m[07/09 01:55:50 d2.data.build]: \u001B[0mUsing training sampler TrainingSampler\n",
      "\u001B[32m[07/09 01:55:50 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/09 01:55:50 d2.data.common]: \u001B[0mSerializing 374 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/09 01:55:50 d2.data.common]: \u001B[0mSerialized dataset takes 0.24 MiB\n",
      "\u001B[32m[07/09 01:55:50 d2.data.build]: \u001B[0mMaking batched data loader with batch_size=4\n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/09 01:55:50 d2.solver.build]: \u001B[0mSOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.\n",
      "\u001B[32m[07/09 01:55:50 d2.checkpoint.detection_checkpoint]: \u001B[0m[DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x/138205316/model_final_a3ec72.pkl ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (4, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (4,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (12, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (12,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (3, 256, 1, 1) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (3,) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001B[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001B[0m\n",
      "\u001B[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001B[0m\n",
      "\u001B[34mroi_heads.mask_head.predictor.{bias, weight}\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[07/09 01:55:50 d2.engine.train_loop]: \u001B[0mStarting training from iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3484.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[07/09 01:56:18 d2.utils.events]: \u001B[0m eta: 0:55:28  iter: 19  total_loss: 2.148  loss_cls: 1.335  loss_box_reg: 0.06444  loss_mask: 0.6898  loss_rpn_cls: 0.04446  loss_rpn_loc: 0.01113    time: 1.0893  last_time: 1.1258  data_time: 0.1989  last_data_time: 0.0031   lr: 1.9981e-06  max_mem: 4867M\n",
      "\u001B[32m[07/09 01:56:41 d2.utils.events]: \u001B[0m eta: 0:53:46  iter: 39  total_loss: 2.136  loss_cls: 1.295  loss_box_reg: 0.07346  loss_mask: 0.6883  loss_rpn_cls: 0.05139  loss_rpn_loc: 0.01122    time: 1.0032  last_time: 1.0939  data_time: 0.0036  last_data_time: 0.0033   lr: 3.9961e-06  max_mem: 4867M\n",
      "\u001B[32m[07/09 01:57:02 d2.utils.events]: \u001B[0m eta: 0:53:29  iter: 59  total_loss: 1.994  loss_cls: 1.169  loss_box_reg: 0.07756  loss_mask: 0.6851  loss_rpn_cls: 0.04408  loss_rpn_loc: 0.008652    time: 1.0192  last_time: 1.0750  data_time: 0.0039  last_data_time: 0.0029   lr: 5.9941e-06  max_mem: 4867M\n",
      "\u001B[32m[07/09 01:57:25 d2.utils.events]: \u001B[0m eta: 0:53:16  iter: 79  total_loss: 1.846  loss_cls: 1.012  loss_box_reg: 0.07988  loss_mask: 0.6806  loss_rpn_cls: 0.04354  loss_rpn_loc: 0.01139    time: 1.0555  last_time: 0.5646  data_time: 0.0036  last_data_time: 0.0036   lr: 7.9921e-06  max_mem: 4970M\n",
      "\u001B[32m[07/09 01:57:45 d2.utils.events]: \u001B[0m eta: 0:52:54  iter: 99  total_loss: 1.652  loss_cls: 0.8236  loss_box_reg: 0.08488  loss_mask: 0.6745  loss_rpn_cls: 0.04366  loss_rpn_loc: 0.01052    time: 1.0372  last_time: 0.5672  data_time: 0.0036  last_data_time: 0.0027   lr: 9.9901e-06  max_mem: 4970M\n",
      "\u001B[32m[07/09 01:58:08 d2.utils.events]: \u001B[0m eta: 0:52:35  iter: 119  total_loss: 1.465  loss_cls: 0.6704  loss_box_reg: 0.07966  loss_mask: 0.6656  loss_rpn_cls: 0.05197  loss_rpn_loc: 0.01292    time: 1.0572  last_time: 1.1823  data_time: 0.0039  last_data_time: 0.0037   lr: 1.1988e-05  max_mem: 4970M\n",
      "\u001B[32m[07/09 01:58:30 d2.utils.events]: \u001B[0m eta: 0:52:27  iter: 139  total_loss: 1.258  loss_cls: 0.4558  loss_box_reg: 0.09338  loss_mask: 0.6577  loss_rpn_cls: 0.03278  loss_rpn_loc: 0.009909    time: 1.0649  last_time: 0.7660  data_time: 0.0039  last_data_time: 0.0041   lr: 1.3986e-05  max_mem: 5091M\n",
      "\u001B[32m[07/09 01:58:57 d2.utils.events]: \u001B[0m eta: 0:52:34  iter: 159  total_loss: 1.163  loss_cls: 0.3616  loss_box_reg: 0.09356  loss_mask: 0.6477  loss_rpn_cls: 0.03267  loss_rpn_loc: 0.009513    time: 1.0994  last_time: 0.8575  data_time: 0.0048  last_data_time: 0.0059   lr: 1.5984e-05  max_mem: 5091M\n",
      "\u001B[32m[07/09 01:59:25 d2.utils.events]: \u001B[0m eta: 0:52:39  iter: 179  total_loss: 1.059  loss_cls: 0.2678  loss_box_reg: 0.09573  loss_mask: 0.6346  loss_rpn_cls: 0.0296  loss_rpn_loc: 0.008687    time: 1.1368  last_time: 1.3967  data_time: 0.0041  last_data_time: 0.0031   lr: 1.7982e-05  max_mem: 5091M\n",
      "\u001B[32m[07/09 01:59:50 d2.utils.events]: \u001B[0m eta: 0:52:26  iter: 199  total_loss: 1.026  loss_cls: 0.238  loss_box_reg: 0.1065  loss_mask: 0.6241  loss_rpn_cls: 0.04067  loss_rpn_loc: 0.009699    time: 1.1443  last_time: 0.6714  data_time: 0.0057  last_data_time: 0.0103   lr: 1.998e-05  max_mem: 5091M\n",
      "\u001B[32m[07/09 02:00:11 d2.utils.events]: \u001B[0m eta: 0:52:08  iter: 219  total_loss: 0.9707  loss_cls: 0.2071  loss_box_reg: 0.1076  loss_mask: 0.5994  loss_rpn_cls: 0.0206  loss_rpn_loc: 0.01015    time: 1.1395  last_time: 1.2458  data_time: 0.0047  last_data_time: 0.0042   lr: 2.1978e-05  max_mem: 5091M\n",
      "\u001B[32m[07/09 02:00:34 d2.utils.events]: \u001B[0m eta: 0:52:21  iter: 239  total_loss: 0.9475  loss_cls: 0.2073  loss_box_reg: 0.1169  loss_mask: 0.5858  loss_rpn_cls: 0.02654  loss_rpn_loc: 0.009554    time: 1.1402  last_time: 0.6324  data_time: 0.0044  last_data_time: 0.0062   lr: 2.3976e-05  max_mem: 5091M\n",
      "\u001B[32m[07/09 02:00:58 d2.utils.events]: \u001B[0m eta: 0:51:58  iter: 259  total_loss: 0.9247  loss_cls: 0.1954  loss_box_reg: 0.1279  loss_mask: 0.5602  loss_rpn_cls: 0.02654  loss_rpn_loc: 0.009375    time: 1.1441  last_time: 1.0694  data_time: 0.0039  last_data_time: 0.0036   lr: 2.5974e-05  max_mem: 5091M\n",
      "\u001B[32m[07/09 02:01:18 d2.utils.events]: \u001B[0m eta: 0:51:38  iter: 279  total_loss: 0.8981  loss_cls: 0.1952  loss_box_reg: 0.1322  loss_mask: 0.5268  loss_rpn_cls: 0.02701  loss_rpn_loc: 0.0102    time: 1.1329  last_time: 1.1429  data_time: 0.0037  last_data_time: 0.0046   lr: 2.7972e-05  max_mem: 5091M\n",
      "\u001B[32m[07/09 02:01:40 d2.utils.events]: \u001B[0m eta: 0:51:35  iter: 299  total_loss: 0.8828  loss_cls: 0.1982  loss_box_reg: 0.1391  loss_mask: 0.5065  loss_rpn_cls: 0.01944  loss_rpn_loc: 0.008679    time: 1.1309  last_time: 0.6929  data_time: 0.0037  last_data_time: 0.0044   lr: 2.997e-05  max_mem: 5091M\n",
      "\u001B[32m[07/09 02:02:01 d2.utils.events]: \u001B[0m eta: 0:51:12  iter: 319  total_loss: 0.8562  loss_cls: 0.1904  loss_box_reg: 0.1473  loss_mask: 0.4928  loss_rpn_cls: 0.01636  loss_rpn_loc: 0.007288    time: 1.1260  last_time: 1.1662  data_time: 0.0038  last_data_time: 0.0036   lr: 3.1968e-05  max_mem: 5098M\n",
      "\u001B[32m[07/09 02:02:22 d2.utils.events]: \u001B[0m eta: 0:50:51  iter: 339  total_loss: 0.8223  loss_cls: 0.1766  loss_box_reg: 0.1597  loss_mask: 0.4476  loss_rpn_cls: 0.01643  loss_rpn_loc: 0.008264    time: 1.1226  last_time: 1.1527  data_time: 0.0040  last_data_time: 0.0036   lr: 3.3966e-05  max_mem: 5098M\n",
      "\u001B[32m[07/09 02:02:44 d2.utils.events]: \u001B[0m eta: 0:50:28  iter: 359  total_loss: 0.8164  loss_cls: 0.1813  loss_box_reg: 0.1753  loss_mask: 0.4169  loss_rpn_cls: 0.01336  loss_rpn_loc: 0.008601    time: 1.1191  last_time: 1.2065  data_time: 0.0037  last_data_time: 0.0034   lr: 3.5964e-05  max_mem: 5101M\n",
      "\u001B[32m[07/09 02:03:06 d2.utils.events]: \u001B[0m eta: 0:50:12  iter: 379  total_loss: 0.8051  loss_cls: 0.1886  loss_box_reg: 0.1825  loss_mask: 0.4085  loss_rpn_cls: 0.0143  loss_rpn_loc: 0.008077    time: 1.1199  last_time: 1.1990  data_time: 0.0037  last_data_time: 0.0034   lr: 3.7962e-05  max_mem: 5101M\n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/09 02:03:28 d2.data.datasets.coco]: \u001B[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001B[32m[07/09 02:03:28 d2.data.datasets.coco]: \u001B[0mLoaded 54 images in COCO format from C:\\Users\\Daanish Mittal\\OneDrive\\Desktop\\Tank_align\\tank_alignment\\army_ramp\\loader.ramp-2\\test\\_annotations.coco.json\n",
      "\u001B[32m[07/09 02:03:28 d2.data.build]: \u001B[0mDistribution of instances among all 2 categories:\n",
      "\u001B[36m|  category   | #instances   |  category  | #instances   |\n",
      "|:-----------:|:-------------|:----------:|:-------------|\n",
      "| loader-ramp | 0            |    ramp    | 97           |\n",
      "|             |              |            |              |\n",
      "|    total    | 97           |            |              |\u001B[0m\n",
      "\u001B[32m[07/09 02:03:28 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001B[32m[07/09 02:03:28 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/09 02:03:28 d2.data.common]: \u001B[0mSerializing 54 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/09 02:03:28 d2.data.common]: \u001B[0mSerialized dataset takes 0.04 MiB\n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/09 02:03:28 d2.engine.defaults]: \u001B[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001B[32m[07/09 02:03:28 d2.utils.events]: \u001B[0m eta: 0:49:50  iter: 399  total_loss: 0.772  loss_cls: 0.1753  loss_box_reg: 0.1758  loss_mask: 0.3747  loss_rpn_cls: 0.01446  loss_rpn_loc: 0.008439    time: 1.1173  last_time: 1.3531  data_time: 0.0044  last_data_time: 0.0047   lr: 3.996e-05  max_mem: 5101M\n",
      "\u001B[32m[07/09 02:03:49 d2.utils.events]: \u001B[0m eta: 0:49:26  iter: 419  total_loss: 0.7489  loss_cls: 0.1733  loss_box_reg: 0.1982  loss_mask: 0.34  loss_rpn_cls: 0.01287  loss_rpn_loc: 0.008062    time: 1.1155  last_time: 0.6958  data_time: 0.0034  last_data_time: 0.0030   lr: 4.1958e-05  max_mem: 5101M\n",
      "\u001B[32m[07/09 02:04:12 d2.utils.events]: \u001B[0m eta: 0:49:00  iter: 439  total_loss: 0.7269  loss_cls: 0.1677  loss_box_reg: 0.2027  loss_mask: 0.3264  loss_rpn_cls: 0.01088  loss_rpn_loc: 0.007354    time: 1.1164  last_time: 1.1902  data_time: 0.0033  last_data_time: 0.0029   lr: 4.3956e-05  max_mem: 5172M\n",
      "\u001B[32m[07/09 02:04:33 d2.utils.events]: \u001B[0m eta: 0:48:41  iter: 459  total_loss: 0.6631  loss_cls: 0.152  loss_box_reg: 0.1987  loss_mask: 0.3038  loss_rpn_cls: 0.01035  loss_rpn_loc: 0.006745    time: 1.1140  last_time: 0.6342  data_time: 0.0034  last_data_time: 0.0041   lr: 4.5954e-05  max_mem: 5232M\n",
      "\u001B[32m[07/09 02:04:55 d2.utils.events]: \u001B[0m eta: 0:48:18  iter: 479  total_loss: 0.6656  loss_cls: 0.1566  loss_box_reg: 0.1934  loss_mask: 0.2937  loss_rpn_cls: 0.007506  loss_rpn_loc: 0.006231    time: 1.1128  last_time: 1.2623  data_time: 0.0036  last_data_time: 0.0050   lr: 4.7952e-05  max_mem: 5232M\n",
      "\u001B[32m[07/09 02:05:18 d2.utils.events]: \u001B[0m eta: 0:47:57  iter: 499  total_loss: 0.6559  loss_cls: 0.1394  loss_box_reg: 0.2036  loss_mask: 0.2863  loss_rpn_cls: 0.008403  loss_rpn_loc: 0.006427    time: 1.1142  last_time: 1.1776  data_time: 0.0034  last_data_time: 0.0030   lr: 4.995e-05  max_mem: 5232M\n",
      "\u001B[32m[07/09 02:05:41 d2.utils.events]: \u001B[0m eta: 0:47:34  iter: 519  total_loss: 0.6421  loss_cls: 0.1375  loss_box_reg: 0.1975  loss_mask: 0.2585  loss_rpn_cls: 0.007556  loss_rpn_loc: 0.006472    time: 1.1151  last_time: 1.1831  data_time: 0.0032  last_data_time: 0.0029   lr: 5.1948e-05  max_mem: 5232M\n",
      "\u001B[32m[07/09 02:06:01 d2.utils.events]: \u001B[0m eta: 0:47:11  iter: 539  total_loss: 0.6337  loss_cls: 0.1386  loss_box_reg: 0.2153  loss_mask: 0.2563  loss_rpn_cls: 0.006376  loss_rpn_loc: 0.006024    time: 1.1115  last_time: 0.6143  data_time: 0.0033  last_data_time: 0.0034   lr: 5.3946e-05  max_mem: 5232M\n",
      "\u001B[32m[07/09 02:06:24 d2.utils.events]: \u001B[0m eta: 0:46:51  iter: 559  total_loss: 0.5978  loss_cls: 0.1315  loss_box_reg: 0.2144  loss_mask: 0.221  loss_rpn_cls: 0.01075  loss_rpn_loc: 0.006544    time: 1.1130  last_time: 0.7090  data_time: 0.0038  last_data_time: 0.0036   lr: 5.5944e-05  max_mem: 5232M\n",
      "\u001B[32m[07/09 02:06:48 d2.utils.events]: \u001B[0m eta: 0:46:32  iter: 579  total_loss: 0.5822  loss_cls: 0.1153  loss_box_reg: 0.2024  loss_mask: 0.2413  loss_rpn_cls: 0.005952  loss_rpn_loc: 0.006217    time: 1.1160  last_time: 1.1796  data_time: 0.0038  last_data_time: 0.0030   lr: 5.7942e-05  max_mem: 5232M\n",
      "\u001B[32m[07/09 02:07:10 d2.utils.events]: \u001B[0m eta: 0:46:12  iter: 599  total_loss: 0.5983  loss_cls: 0.114  loss_box_reg: 0.2136  loss_mask: 0.2298  loss_rpn_cls: 0.006719  loss_rpn_loc: 0.006349    time: 1.1158  last_time: 0.6980  data_time: 0.0039  last_data_time: 0.0055   lr: 5.994e-05  max_mem: 5263M\n",
      "\u001B[32m[07/09 02:07:35 d2.utils.events]: \u001B[0m eta: 0:45:55  iter: 619  total_loss: 0.5518  loss_cls: 0.1066  loss_box_reg: 0.2176  loss_mask: 0.2151  loss_rpn_cls: 0.006134  loss_rpn_loc: 0.006533    time: 1.1187  last_time: 1.1699  data_time: 0.0035  last_data_time: 0.0032   lr: 6.1938e-05  max_mem: 5263M\n",
      "\u001B[32m[07/09 02:07:56 d2.utils.events]: \u001B[0m eta: 0:45:35  iter: 639  total_loss: 0.5372  loss_cls: 0.09833  loss_box_reg: 0.2038  loss_mask: 0.1971  loss_rpn_cls: 0.008683  loss_rpn_loc: 0.006059    time: 1.1181  last_time: 1.0767  data_time: 0.0033  last_data_time: 0.0030   lr: 6.3936e-05  max_mem: 5263M\n",
      "\u001B[32m[07/09 02:08:20 d2.utils.events]: \u001B[0m eta: 0:45:11  iter: 659  total_loss: 0.5261  loss_cls: 0.09046  loss_box_reg: 0.1961  loss_mask: 0.2104  loss_rpn_cls: 0.003232  loss_rpn_loc: 0.005345    time: 1.1192  last_time: 1.1697  data_time: 0.0034  last_data_time: 0.0030   lr: 6.5934e-05  max_mem: 5263M\n",
      "\u001B[32m[07/09 02:08:40 d2.utils.events]: \u001B[0m eta: 0:44:42  iter: 679  total_loss: 0.5264  loss_cls: 0.09048  loss_box_reg: 0.2062  loss_mask: 0.1975  loss_rpn_cls: 0.004713  loss_rpn_loc: 0.005903    time: 1.1157  last_time: 1.0843  data_time: 0.0032  last_data_time: 0.0035   lr: 6.7932e-05  max_mem: 5263M\n",
      "\u001B[32m[07/09 02:09:00 d2.utils.events]: \u001B[0m eta: 0:44:15  iter: 699  total_loss: 0.5067  loss_cls: 0.08598  loss_box_reg: 0.2032  loss_mask: 0.1947  loss_rpn_cls: 0.003775  loss_rpn_loc: 0.006696    time: 1.1130  last_time: 0.6320  data_time: 0.0031  last_data_time: 0.0031   lr: 6.993e-05  max_mem: 5263M\n",
      "\u001B[32m[07/09 02:09:19 d2.utils.events]: \u001B[0m eta: 0:43:50  iter: 719  total_loss: 0.4969  loss_cls: 0.08677  loss_box_reg: 0.2043  loss_mask: 0.1973  loss_rpn_cls: 0.004876  loss_rpn_loc: 0.005635    time: 1.1090  last_time: 0.7182  data_time: 0.0032  last_data_time: 0.0028   lr: 7.1928e-05  max_mem: 5263M\n",
      "\u001B[32m[07/09 02:09:42 d2.utils.events]: \u001B[0m eta: 0:43:27  iter: 739  total_loss: 0.47  loss_cls: 0.07692  loss_box_reg: 0.1891  loss_mask: 0.1969  loss_rpn_cls: 0.003952  loss_rpn_loc: 0.00531    time: 1.1101  last_time: 1.1059  data_time: 0.0035  last_data_time: 0.0037   lr: 7.3926e-05  max_mem: 5263M\n",
      "\u001B[32m[07/09 02:10:03 d2.utils.events]: \u001B[0m eta: 0:43:04  iter: 759  total_loss: 0.4714  loss_cls: 0.07338  loss_box_reg: 0.1865  loss_mask: 0.19  loss_rpn_cls: 0.003297  loss_rpn_loc: 0.005692    time: 1.1083  last_time: 1.0660  data_time: 0.0032  last_data_time: 0.0033   lr: 7.5924e-05  max_mem: 5263M\n",
      "\u001B[32m[07/09 02:10:23 d2.utils.events]: \u001B[0m eta: 0:42:38  iter: 779  total_loss: 0.4962  loss_cls: 0.07541  loss_box_reg: 0.174  loss_mask: 0.1778  loss_rpn_cls: 0.006153  loss_rpn_loc: 0.006021    time: 1.1051  last_time: 0.6234  data_time: 0.0029  last_data_time: 0.0029   lr: 7.7922e-05  max_mem: 5263M\n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/09 02:10:43 d2.data.datasets.coco]: \u001B[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001B[32m[07/09 02:10:43 d2.data.datasets.coco]: \u001B[0mLoaded 54 images in COCO format from C:\\Users\\Daanish Mittal\\OneDrive\\Desktop\\Tank_align\\tank_alignment\\army_ramp\\loader.ramp-2\\test\\_annotations.coco.json\n",
      "\u001B[32m[07/09 02:10:44 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001B[32m[07/09 02:10:44 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/09 02:10:44 d2.data.common]: \u001B[0mSerializing 54 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/09 02:10:44 d2.data.common]: \u001B[0mSerialized dataset takes 0.04 MiB\n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/09 02:10:44 d2.engine.defaults]: \u001B[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001B[32m[07/09 02:10:44 d2.utils.events]: \u001B[0m eta: 0:42:13  iter: 799  total_loss: 0.4591  loss_cls: 0.06728  loss_box_reg: 0.1776  loss_mask: 0.1817  loss_rpn_cls: 0.00392  loss_rpn_loc: 0.00602    time: 1.1031  last_time: 0.7126  data_time: 0.0032  last_data_time: 0.0030   lr: 7.992e-05  max_mem: 5329M\n",
      "\u001B[32m[07/09 02:11:06 d2.utils.events]: \u001B[0m eta: 0:41:48  iter: 819  total_loss: 0.4664  loss_cls: 0.06021  loss_box_reg: 0.1548  loss_mask: 0.2085  loss_rpn_cls: 0.003083  loss_rpn_loc: 0.004777    time: 1.1042  last_time: 1.2068  data_time: 0.0031  last_data_time: 0.0029   lr: 8.1918e-05  max_mem: 5329M\n",
      "\u001B[32m[07/09 02:11:27 d2.utils.events]: \u001B[0m eta: 0:41:23  iter: 839  total_loss: 0.4414  loss_cls: 0.06971  loss_box_reg: 0.1544  loss_mask: 0.1902  loss_rpn_cls: 0.004872  loss_rpn_loc: 0.0063    time: 1.1027  last_time: 0.6969  data_time: 0.0031  last_data_time: 0.0029   lr: 8.3916e-05  max_mem: 5329M\n",
      "\u001B[32m[07/09 02:11:46 d2.utils.events]: \u001B[0m eta: 0:40:56  iter: 859  total_loss: 0.4145  loss_cls: 0.05449  loss_box_reg: 0.1497  loss_mask: 0.1634  loss_rpn_cls: 0.00469  loss_rpn_loc: 0.005743    time: 1.0992  last_time: 0.6995  data_time: 0.0031  last_data_time: 0.0031   lr: 8.5914e-05  max_mem: 5329M\n",
      "\u001B[32m[07/09 02:12:06 d2.utils.events]: \u001B[0m eta: 0:40:31  iter: 879  total_loss: 0.3879  loss_cls: 0.06231  loss_box_reg: 0.1473  loss_mask: 0.1758  loss_rpn_cls: 0.002742  loss_rpn_loc: 0.006053    time: 1.0970  last_time: 1.1764  data_time: 0.0033  last_data_time: 0.0035   lr: 8.7912e-05  max_mem: 5329M\n",
      "\u001B[32m[07/09 02:12:29 d2.utils.events]: \u001B[0m eta: 0:40:06  iter: 899  total_loss: 0.3934  loss_cls: 0.05248  loss_box_reg: 0.1302  loss_mask: 0.1888  loss_rpn_cls: 0.002688  loss_rpn_loc: 0.005143    time: 1.0972  last_time: 1.0701  data_time: 0.0032  last_data_time: 0.0032   lr: 8.991e-05  max_mem: 5329M\n",
      "\u001B[32m[07/09 02:12:50 d2.utils.events]: \u001B[0m eta: 0:39:40  iter: 919  total_loss: 0.3756  loss_cls: 0.05409  loss_box_reg: 0.1257  loss_mask: 0.1876  loss_rpn_cls: 0.002391  loss_rpn_loc: 0.005165    time: 1.0967  last_time: 1.0712  data_time: 0.0031  last_data_time: 0.0033   lr: 9.1908e-05  max_mem: 5329M\n",
      "\u001B[32m[07/09 02:13:11 d2.utils.events]: \u001B[0m eta: 0:39:16  iter: 939  total_loss: 0.386  loss_cls: 0.05062  loss_box_reg: 0.1194  loss_mask: 0.193  loss_rpn_cls: 0.003659  loss_rpn_loc: 0.005506    time: 1.0961  last_time: 1.1567  data_time: 0.0030  last_data_time: 0.0028   lr: 9.3906e-05  max_mem: 5329M\n",
      "\u001B[32m[07/09 02:13:33 d2.utils.events]: \u001B[0m eta: 0:38:55  iter: 959  total_loss: 0.3604  loss_cls: 0.05072  loss_box_reg: 0.1116  loss_mask: 0.1839  loss_rpn_cls: 0.001431  loss_rpn_loc: 0.005193    time: 1.0962  last_time: 1.3153  data_time: 0.0032  last_data_time: 0.0036   lr: 9.5904e-05  max_mem: 5329M\n",
      "\u001B[32m[07/09 02:13:55 d2.utils.events]: \u001B[0m eta: 0:38:32  iter: 979  total_loss: 0.3947  loss_cls: 0.06111  loss_box_reg: 0.116  loss_mask: 0.1705  loss_rpn_cls: 0.003773  loss_rpn_loc: 0.005273    time: 1.0964  last_time: 1.0702  data_time: 0.0033  last_data_time: 0.0029   lr: 9.7902e-05  max_mem: 5329M\n",
      "\u001B[32m[07/09 02:14:16 d2.utils.events]: \u001B[0m eta: 0:38:07  iter: 999  total_loss: 0.3334  loss_cls: 0.04572  loss_box_reg: 0.1019  loss_mask: 0.1645  loss_rpn_cls: 0.001078  loss_rpn_loc: 0.004767    time: 1.0951  last_time: 1.0635  data_time: 0.0030  last_data_time: 0.0031   lr: 9.99e-05  max_mem: 5329M\n",
      "\u001B[32m[07/09 02:14:38 d2.utils.events]: \u001B[0m eta: 0:37:43  iter: 1019  total_loss: 0.3044  loss_cls: 0.04542  loss_box_reg: 0.08904  loss_mask: 0.1678  loss_rpn_cls: 0.003847  loss_rpn_loc: 0.004682    time: 1.0952  last_time: 1.0733  data_time: 0.0031  last_data_time: 0.0029   lr: 0.0001  max_mem: 5329M\n",
      "\u001B[32m[07/09 02:15:02 d2.utils.events]: \u001B[0m eta: 0:37:25  iter: 1039  total_loss: 0.3381  loss_cls: 0.04586  loss_box_reg: 0.09764  loss_mask: 0.1754  loss_rpn_cls: 0.001168  loss_rpn_loc: 0.004838    time: 1.0972  last_time: 1.1537  data_time: 0.0032  last_data_time: 0.0031   lr: 0.0001  max_mem: 5329M\n",
      "\u001B[32m[07/09 02:15:23 d2.utils.events]: \u001B[0m eta: 0:37:04  iter: 1059  total_loss: 0.3324  loss_cls: 0.04658  loss_box_reg: 0.08987  loss_mask: 0.1722  loss_rpn_cls: 0.003407  loss_rpn_loc: 0.005224    time: 1.0958  last_time: 1.0362  data_time: 0.0032  last_data_time: 0.0022   lr: 0.0001  max_mem: 5329M\n",
      "\u001B[32m[07/09 02:15:43 d2.utils.events]: \u001B[0m eta: 0:36:42  iter: 1079  total_loss: 0.3629  loss_cls: 0.04471  loss_box_reg: 0.09522  loss_mask: 0.1985  loss_rpn_cls: 0.002157  loss_rpn_loc: 0.005237    time: 1.0941  last_time: 1.1825  data_time: 0.0030  last_data_time: 0.0028   lr: 0.0001  max_mem: 5329M\n",
      "\u001B[32m[07/09 02:16:04 d2.utils.events]: \u001B[0m eta: 0:36:20  iter: 1099  total_loss: 0.3216  loss_cls: 0.04166  loss_box_reg: 0.08795  loss_mask: 0.1941  loss_rpn_cls: 0.00193  loss_rpn_loc: 0.004856    time: 1.0934  last_time: 0.7159  data_time: 0.0031  last_data_time: 0.0034   lr: 0.0001  max_mem: 5329M\n",
      "\u001B[32m[07/09 02:16:25 d2.utils.events]: \u001B[0m eta: 0:35:56  iter: 1119  total_loss: 0.3044  loss_cls: 0.03825  loss_box_reg: 0.0832  loss_mask: 0.1689  loss_rpn_cls: 0.002281  loss_rpn_loc: 0.005034    time: 1.0930  last_time: 1.1827  data_time: 0.0031  last_data_time: 0.0031   lr: 0.0001  max_mem: 5329M\n",
      "\u001B[32m[07/09 02:16:48 d2.utils.events]: \u001B[0m eta: 0:35:32  iter: 1139  total_loss: 0.2882  loss_cls: 0.03987  loss_box_reg: 0.08088  loss_mask: 0.1648  loss_rpn_cls: 0.001953  loss_rpn_loc: 0.004319    time: 1.0933  last_time: 1.0599  data_time: 0.0030  last_data_time: 0.0034   lr: 0.0001  max_mem: 5329M\n",
      "\u001B[32m[07/09 02:17:08 d2.utils.events]: \u001B[0m eta: 0:35:04  iter: 1159  total_loss: 0.2973  loss_cls: 0.04302  loss_box_reg: 0.08286  loss_mask: 0.1668  loss_rpn_cls: 0.002164  loss_rpn_loc: 0.004546    time: 1.0924  last_time: 1.0427  data_time: 0.0032  last_data_time: 0.0028   lr: 0.0001  max_mem: 5329M\n",
      "\u001B[32m[07/09 02:17:31 d2.utils.events]: \u001B[0m eta: 0:34:36  iter: 1179  total_loss: 0.3583  loss_cls: 0.04093  loss_box_reg: 0.09808  loss_mask: 0.1951  loss_rpn_cls: 0.002707  loss_rpn_loc: 0.005816    time: 1.0927  last_time: 1.1171  data_time: 0.0031  last_data_time: 0.0029   lr: 0.0001  max_mem: 5329M\n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/09 02:17:50 d2.data.datasets.coco]: \u001B[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001B[32m[07/09 02:17:50 d2.data.datasets.coco]: \u001B[0mLoaded 54 images in COCO format from C:\\Users\\Daanish Mittal\\OneDrive\\Desktop\\Tank_align\\tank_alignment\\army_ramp\\loader.ramp-2\\test\\_annotations.coco.json\n",
      "\u001B[32m[07/09 02:17:50 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001B[32m[07/09 02:17:50 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/09 02:17:50 d2.data.common]: \u001B[0mSerializing 54 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/09 02:17:50 d2.data.common]: \u001B[0mSerialized dataset takes 0.04 MiB\n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/09 02:17:50 d2.engine.defaults]: \u001B[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001B[32m[07/09 02:17:50 d2.utils.events]: \u001B[0m eta: 0:34:11  iter: 1199  total_loss: 0.2672  loss_cls: 0.03812  loss_box_reg: 0.07936  loss_mask: 0.1408  loss_rpn_cls: 0.002161  loss_rpn_loc: 0.0042    time: 1.0910  last_time: 1.0534  data_time: 0.0031  last_data_time: 0.0030   lr: 0.0001  max_mem: 5329M\n",
      "\u001B[32m[07/09 02:18:10 d2.utils.events]: \u001B[0m eta: 0:33:48  iter: 1219  total_loss: 0.3235  loss_cls: 0.04616  loss_box_reg: 0.08468  loss_mask: 0.1686  loss_rpn_cls: 0.001484  loss_rpn_loc: 0.004989    time: 1.0896  last_time: 0.7113  data_time: 0.0031  last_data_time: 0.0030   lr: 0.0001  max_mem: 5329M\n",
      "\u001B[32m[07/09 02:18:33 d2.utils.events]: \u001B[0m eta: 0:33:24  iter: 1239  total_loss: 0.3271  loss_cls: 0.04291  loss_box_reg: 0.08443  loss_mask: 0.1975  loss_rpn_cls: 0.002414  loss_rpn_loc: 0.004955    time: 1.0906  last_time: 1.1563  data_time: 0.0030  last_data_time: 0.0030   lr: 0.0001  max_mem: 5329M\n",
      "\u001B[32m[07/09 02:18:54 d2.utils.events]: \u001B[0m eta: 0:32:57  iter: 1259  total_loss: 0.2843  loss_cls: 0.03551  loss_box_reg: 0.07664  loss_mask: 0.1662  loss_rpn_cls: 0.002087  loss_rpn_loc: 0.004478    time: 1.0899  last_time: 1.0573  data_time: 0.0032  last_data_time: 0.0026   lr: 0.0001  max_mem: 5329M\n",
      "\u001B[32m[07/09 02:19:16 d2.utils.events]: \u001B[0m eta: 0:32:33  iter: 1279  total_loss: 0.2965  loss_cls: 0.03599  loss_box_reg: 0.08085  loss_mask: 0.1543  loss_rpn_cls: 0.002016  loss_rpn_loc: 0.004498    time: 1.0901  last_time: 1.0473  data_time: 0.0031  last_data_time: 0.0027   lr: 0.0001  max_mem: 5329M\n",
      "\u001B[32m[07/09 02:19:39 d2.utils.events]: \u001B[0m eta: 0:32:05  iter: 1299  total_loss: 0.2724  loss_cls: 0.03227  loss_box_reg: 0.07464  loss_mask: 0.1637  loss_rpn_cls: 0.00122  loss_rpn_loc: 0.004133    time: 1.0904  last_time: 1.1271  data_time: 0.0030  last_data_time: 0.0029   lr: 0.0001  max_mem: 5329M\n",
      "\u001B[32m[07/09 02:20:00 d2.utils.events]: \u001B[0m eta: 0:31:42  iter: 1319  total_loss: 0.2903  loss_cls: 0.03428  loss_box_reg: 0.07591  loss_mask: 0.1616  loss_rpn_cls: 0.0009796  loss_rpn_loc: 0.004326    time: 1.0903  last_time: 1.1728  data_time: 0.0030  last_data_time: 0.0030   lr: 0.0001  max_mem: 5329M\n",
      "\u001B[32m[07/09 02:20:24 d2.utils.events]: \u001B[0m eta: 0:31:20  iter: 1339  total_loss: 0.2914  loss_cls: 0.03315  loss_box_reg: 0.07578  loss_mask: 0.1726  loss_rpn_cls: 0.001613  loss_rpn_loc: 0.004567    time: 1.0915  last_time: 1.0512  data_time: 0.0034  last_data_time: 0.0031   lr: 0.0001  max_mem: 5329M\n",
      "\u001B[32m[07/09 02:20:44 d2.utils.events]: \u001B[0m eta: 0:30:54  iter: 1359  total_loss: 0.2693  loss_cls: 0.03509  loss_box_reg: 0.0729  loss_mask: 0.1719  loss_rpn_cls: 0.001853  loss_rpn_loc: 0.003963    time: 1.0900  last_time: 1.0667  data_time: 0.0031  last_data_time: 0.0026   lr: 0.0001  max_mem: 5329M\n",
      "\u001B[32m[07/09 02:21:06 d2.utils.events]: \u001B[0m eta: 0:30:30  iter: 1379  total_loss: 0.3082  loss_cls: 0.04192  loss_box_reg: 0.07391  loss_mask: 0.1632  loss_rpn_cls: 0.00171  loss_rpn_loc: 0.004873    time: 1.0904  last_time: 1.1628  data_time: 0.0031  last_data_time: 0.0041   lr: 0.0001  max_mem: 5329M\n",
      "\u001B[32m[07/09 02:21:25 d2.utils.events]: \u001B[0m eta: 0:30:07  iter: 1399  total_loss: 0.3045  loss_cls: 0.03199  loss_box_reg: 0.07317  loss_mask: 0.1848  loss_rpn_cls: 0.0009476  loss_rpn_loc: 0.004166    time: 1.0888  last_time: 1.1533  data_time: 0.0032  last_data_time: 0.0028   lr: 0.0001  max_mem: 5329M\n",
      "\u001B[32m[07/09 02:21:47 d2.utils.events]: \u001B[0m eta: 0:29:44  iter: 1419  total_loss: 0.3164  loss_cls: 0.03611  loss_box_reg: 0.08117  loss_mask: 0.1875  loss_rpn_cls: 0.001445  loss_rpn_loc: 0.005061    time: 1.0886  last_time: 1.1589  data_time: 0.0030  last_data_time: 0.0031   lr: 0.0001  max_mem: 5329M\n",
      "\u001B[32m[07/09 02:22:08 d2.utils.events]: \u001B[0m eta: 0:29:22  iter: 1439  total_loss: 0.2598  loss_cls: 0.03375  loss_box_reg: 0.06861  loss_mask: 0.1456  loss_rpn_cls: 0.001592  loss_rpn_loc: 0.004422    time: 1.0882  last_time: 1.0685  data_time: 0.0032  last_data_time: 0.0031   lr: 0.0001  max_mem: 5329M\n",
      "\u001B[32m[07/09 02:22:30 d2.utils.events]: \u001B[0m eta: 0:29:00  iter: 1459  total_loss: 0.3047  loss_cls: 0.03765  loss_box_reg: 0.08003  loss_mask: 0.1859  loss_rpn_cls: 0.0008827  loss_rpn_loc: 0.004628    time: 1.0880  last_time: 0.6993  data_time: 0.0031  last_data_time: 0.0027   lr: 0.0001  max_mem: 5329M\n",
      "\u001B[32m[07/09 02:22:51 d2.utils.events]: \u001B[0m eta: 0:28:37  iter: 1479  total_loss: 0.3032  loss_cls: 0.03213  loss_box_reg: 0.08022  loss_mask: 0.1857  loss_rpn_cls: 0.001082  loss_rpn_loc: 0.004101    time: 1.0877  last_time: 1.0370  data_time: 0.0031  last_data_time: 0.0030   lr: 0.0001  max_mem: 5329M\n",
      "\u001B[32m[07/09 02:23:11 d2.utils.events]: \u001B[0m eta: 0:28:13  iter: 1499  total_loss: 0.2456  loss_cls: 0.02661  loss_box_reg: 0.06447  loss_mask: 0.1425  loss_rpn_cls: 0.001854  loss_rpn_loc: 0.003606    time: 1.0868  last_time: 1.0790  data_time: 0.0032  last_data_time: 0.0024   lr: 0.0001  max_mem: 5329M\n",
      "\u001B[32m[07/09 02:23:32 d2.utils.events]: \u001B[0m eta: 0:27:49  iter: 1519  total_loss: 0.2865  loss_cls: 0.03256  loss_box_reg: 0.07354  loss_mask: 0.1846  loss_rpn_cls: 0.0009304  loss_rpn_loc: 0.004386    time: 1.0858  last_time: 1.0869  data_time: 0.0031  last_data_time: 0.0032   lr: 0.0001  max_mem: 5329M\n",
      "\u001B[32m[07/09 02:23:51 d2.utils.events]: \u001B[0m eta: 0:27:25  iter: 1539  total_loss: 0.2851  loss_cls: 0.03312  loss_box_reg: 0.07837  loss_mask: 0.1845  loss_rpn_cls: 0.001399  loss_rpn_loc: 0.004815    time: 1.0841  last_time: 0.6053  data_time: 0.0032  last_data_time: 0.0034   lr: 0.0001  max_mem: 5329M\n",
      "\u001B[32m[07/09 02:24:11 d2.utils.events]: \u001B[0m eta: 0:27:02  iter: 1559  total_loss: 0.2912  loss_cls: 0.03497  loss_box_reg: 0.07666  loss_mask: 0.1526  loss_rpn_cls: 0.001412  loss_rpn_loc: 0.004297    time: 1.0834  last_time: 0.7025  data_time: 0.0033  last_data_time: 0.0029   lr: 0.0001  max_mem: 5329M\n",
      "\u001B[32m[07/09 02:24:34 d2.utils.events]: \u001B[0m eta: 0:26:35  iter: 1579  total_loss: 0.2616  loss_cls: 0.02944  loss_box_reg: 0.06406  loss_mask: 0.1384  loss_rpn_cls: 0.0009008  loss_rpn_loc: 0.003978    time: 1.0842  last_time: 1.1572  data_time: 0.0030  last_data_time: 0.0029   lr: 0.0001  max_mem: 5329M\n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/09 02:24:56 d2.data.datasets.coco]: \u001B[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001B[32m[07/09 02:24:56 d2.data.datasets.coco]: \u001B[0mLoaded 54 images in COCO format from C:\\Users\\Daanish Mittal\\OneDrive\\Desktop\\Tank_align\\tank_alignment\\army_ramp\\loader.ramp-2\\test\\_annotations.coco.json\n",
      "\u001B[32m[07/09 02:24:56 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001B[32m[07/09 02:24:56 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/09 02:24:56 d2.data.common]: \u001B[0mSerializing 54 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/09 02:24:56 d2.data.common]: \u001B[0mSerialized dataset takes 0.04 MiB\n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/09 02:24:56 d2.engine.defaults]: \u001B[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001B[32m[07/09 02:24:56 d2.utils.events]: \u001B[0m eta: 0:26:12  iter: 1599  total_loss: 0.2523  loss_cls: 0.02845  loss_box_reg: 0.0681  loss_mask: 0.1411  loss_rpn_cls: 0.0006785  loss_rpn_loc: 0.003371    time: 1.0842  last_time: 0.6873  data_time: 0.0033  last_data_time: 0.0026   lr: 0.0001  max_mem: 5329M\n",
      "\u001B[32m[07/09 02:25:18 d2.utils.events]: \u001B[0m eta: 0:25:47  iter: 1619  total_loss: 0.2798  loss_cls: 0.03177  loss_box_reg: 0.06873  loss_mask: 0.1558  loss_rpn_cls: 0.0009692  loss_rpn_loc: 0.004062    time: 1.0846  last_time: 1.0708  data_time: 0.0031  last_data_time: 0.0023   lr: 0.0001  max_mem: 5356M\n",
      "\u001B[32m[07/09 02:25:40 d2.utils.events]: \u001B[0m eta: 0:25:27  iter: 1639  total_loss: 0.2511  loss_cls: 0.03166  loss_box_reg: 0.0682  loss_mask: 0.151  loss_rpn_cls: 0.001435  loss_rpn_loc: 0.004326    time: 1.0849  last_time: 1.1272  data_time: 0.0033  last_data_time: 0.0034   lr: 0.0001  max_mem: 5356M\n",
      "\u001B[32m[07/09 02:26:03 d2.utils.events]: \u001B[0m eta: 0:25:04  iter: 1659  total_loss: 0.2796  loss_cls: 0.03609  loss_box_reg: 0.0754  loss_mask: 0.1544  loss_rpn_cls: 0.001714  loss_rpn_loc: 0.004696    time: 1.0851  last_time: 1.0584  data_time: 0.0031  last_data_time: 0.0032   lr: 0.0001  max_mem: 5356M\n",
      "\u001B[32m[07/09 02:26:26 d2.utils.events]: \u001B[0m eta: 0:24:46  iter: 1679  total_loss: 0.2924  loss_cls: 0.0325  loss_box_reg: 0.07431  loss_mask: 0.1746  loss_rpn_cls: 0.001003  loss_rpn_loc: 0.004674    time: 1.0860  last_time: 1.0330  data_time: 0.0034  last_data_time: 0.0037   lr: 0.0001  max_mem: 5356M\n",
      "\u001B[32m[07/09 02:26:48 d2.utils.events]: \u001B[0m eta: 0:24:25  iter: 1699  total_loss: 0.2657  loss_cls: 0.03072  loss_box_reg: 0.06522  loss_mask: 0.1578  loss_rpn_cls: 0.0009775  loss_rpn_loc: 0.003911    time: 1.0861  last_time: 1.1320  data_time: 0.0032  last_data_time: 0.0034   lr: 0.0001  max_mem: 5356M\n",
      "\u001B[32m[07/09 02:27:07 d2.utils.events]: \u001B[0m eta: 0:24:02  iter: 1719  total_loss: 0.2766  loss_cls: 0.02966  loss_box_reg: 0.06774  loss_mask: 0.1667  loss_rpn_cls: 0.0008476  loss_rpn_loc: 0.004077    time: 1.0849  last_time: 1.0624  data_time: 0.0030  last_data_time: 0.0031   lr: 0.0001  max_mem: 5356M\n",
      "\u001B[32m[07/09 02:27:28 d2.utils.events]: \u001B[0m eta: 0:23:40  iter: 1739  total_loss: 0.2983  loss_cls: 0.03161  loss_box_reg: 0.07011  loss_mask: 0.1797  loss_rpn_cls: 0.001236  loss_rpn_loc: 0.004521    time: 1.0845  last_time: 1.1432  data_time: 0.0031  last_data_time: 0.0031   lr: 0.0001  max_mem: 5356M\n",
      "\u001B[32m[07/09 02:27:49 d2.utils.events]: \u001B[0m eta: 0:23:16  iter: 1759  total_loss: 0.3107  loss_cls: 0.03323  loss_box_reg: 0.07251  loss_mask: 0.1945  loss_rpn_cls: 0.001885  loss_rpn_loc: 0.004239    time: 1.0837  last_time: 0.6332  data_time: 0.0031  last_data_time: 0.0033   lr: 0.0001  max_mem: 5356M\n",
      "\u001B[32m[07/09 02:28:11 d2.utils.events]: \u001B[0m eta: 0:22:55  iter: 1779  total_loss: 0.2858  loss_cls: 0.03063  loss_box_reg: 0.07382  loss_mask: 0.1823  loss_rpn_cls: 0.001111  loss_rpn_loc: 0.004222    time: 1.0839  last_time: 1.1414  data_time: 0.0031  last_data_time: 0.0029   lr: 0.0001  max_mem: 5356M\n",
      "\u001B[32m[07/09 02:28:32 d2.utils.events]: \u001B[0m eta: 0:22:32  iter: 1799  total_loss: 0.2968  loss_cls: 0.03215  loss_box_reg: 0.07335  loss_mask: 0.1793  loss_rpn_cls: 0.001027  loss_rpn_loc: 0.004408    time: 1.0835  last_time: 1.2768  data_time: 0.0032  last_data_time: 0.0035   lr: 0.0001  max_mem: 5356M\n",
      "\u001B[32m[07/09 02:28:54 d2.utils.events]: \u001B[0m eta: 0:22:10  iter: 1819  total_loss: 0.2632  loss_cls: 0.03074  loss_box_reg: 0.05928  loss_mask: 0.15  loss_rpn_cls: 0.001261  loss_rpn_loc: 0.004199    time: 1.0836  last_time: 0.6932  data_time: 0.0032  last_data_time: 0.0032   lr: 0.0001  max_mem: 5356M\n",
      "\u001B[32m[07/09 02:29:16 d2.utils.events]: \u001B[0m eta: 0:21:47  iter: 1839  total_loss: 0.2719  loss_cls: 0.03133  loss_box_reg: 0.06811  loss_mask: 0.161  loss_rpn_cls: 0.0008443  loss_rpn_loc: 0.004222    time: 1.0842  last_time: 1.1540  data_time: 0.0029  last_data_time: 0.0030   lr: 0.0001  max_mem: 5356M\n",
      "\u001B[32m[07/09 02:29:38 d2.utils.events]: \u001B[0m eta: 0:21:25  iter: 1859  total_loss: 0.2771  loss_cls: 0.03076  loss_box_reg: 0.0665  loss_mask: 0.168  loss_rpn_cls: 0.0006114  loss_rpn_loc: 0.004201    time: 1.0842  last_time: 1.3215  data_time: 0.0032  last_data_time: 0.0041   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:30:00 d2.utils.events]: \u001B[0m eta: 0:21:03  iter: 1879  total_loss: 0.2749  loss_cls: 0.03119  loss_box_reg: 0.06906  loss_mask: 0.1604  loss_rpn_cls: 0.001233  loss_rpn_loc: 0.004281    time: 1.0841  last_time: 1.0665  data_time: 0.0032  last_data_time: 0.0026   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:30:19 d2.utils.events]: \u001B[0m eta: 0:20:40  iter: 1899  total_loss: 0.2794  loss_cls: 0.03121  loss_box_reg: 0.06674  loss_mask: 0.1576  loss_rpn_cls: 0.001174  loss_rpn_loc: 0.004231    time: 1.0832  last_time: 1.0561  data_time: 0.0030  last_data_time: 0.0024   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:30:40 d2.utils.events]: \u001B[0m eta: 0:20:17  iter: 1919  total_loss: 0.2574  loss_cls: 0.02788  loss_box_reg: 0.06277  loss_mask: 0.1497  loss_rpn_cls: 0.001228  loss_rpn_loc: 0.003629    time: 1.0825  last_time: 0.6168  data_time: 0.0030  last_data_time: 0.0034   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:31:00 d2.utils.events]: \u001B[0m eta: 0:19:55  iter: 1939  total_loss: 0.3133  loss_cls: 0.03437  loss_box_reg: 0.07945  loss_mask: 0.1924  loss_rpn_cls: 0.0004571  loss_rpn_loc: 0.004393    time: 1.0818  last_time: 1.1466  data_time: 0.0031  last_data_time: 0.0025   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:31:23 d2.utils.events]: \u001B[0m eta: 0:19:32  iter: 1959  total_loss: 0.2461  loss_cls: 0.02963  loss_box_reg: 0.06545  loss_mask: 0.1448  loss_rpn_cls: 0.0008987  loss_rpn_loc: 0.004217    time: 1.0822  last_time: 1.1599  data_time: 0.0031  last_data_time: 0.0032   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:31:43 d2.utils.events]: \u001B[0m eta: 0:19:08  iter: 1979  total_loss: 0.2562  loss_cls: 0.03046  loss_box_reg: 0.06089  loss_mask: 0.1633  loss_rpn_cls: 0.0007498  loss_rpn_loc: 0.004819    time: 1.0816  last_time: 0.7054  data_time: 0.0031  last_data_time: 0.0026   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/09 02:32:04 d2.data.datasets.coco]: \u001B[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001B[32m[07/09 02:32:04 d2.data.datasets.coco]: \u001B[0mLoaded 54 images in COCO format from C:\\Users\\Daanish Mittal\\OneDrive\\Desktop\\Tank_align\\tank_alignment\\army_ramp\\loader.ramp-2\\test\\_annotations.coco.json\n",
      "\u001B[32m[07/09 02:32:04 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001B[32m[07/09 02:32:04 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/09 02:32:04 d2.data.common]: \u001B[0mSerializing 54 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/09 02:32:04 d2.data.common]: \u001B[0mSerialized dataset takes 0.04 MiB\n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/09 02:32:04 d2.engine.defaults]: \u001B[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001B[32m[07/09 02:32:04 d2.utils.events]: \u001B[0m eta: 0:18:46  iter: 1999  total_loss: 0.2466  loss_cls: 0.0332  loss_box_reg: 0.0653  loss_mask: 0.1475  loss_rpn_cls: 0.0005377  loss_rpn_loc: 0.003856    time: 1.0815  last_time: 1.1575  data_time: 0.0033  last_data_time: 0.0032   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:32:26 d2.utils.events]: \u001B[0m eta: 0:18:21  iter: 2019  total_loss: 0.2814  loss_cls: 0.02977  loss_box_reg: 0.06575  loss_mask: 0.1587  loss_rpn_cls: 0.0006922  loss_rpn_loc: 0.004266    time: 1.0816  last_time: 1.2342  data_time: 0.0030  last_data_time: 0.0023   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:32:47 d2.utils.events]: \u001B[0m eta: 0:17:56  iter: 2039  total_loss: 0.2962  loss_cls: 0.0329  loss_box_reg: 0.07136  loss_mask: 0.1673  loss_rpn_cls: 0.0005479  loss_rpn_loc: 0.004385    time: 1.0810  last_time: 1.0599  data_time: 0.0031  last_data_time: 0.0020   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:33:08 d2.utils.events]: \u001B[0m eta: 0:17:33  iter: 2059  total_loss: 0.2442  loss_cls: 0.02418  loss_box_reg: 0.05784  loss_mask: 0.1568  loss_rpn_cls: 0.0002446  loss_rpn_loc: 0.003868    time: 1.0810  last_time: 1.0368  data_time: 0.0030  last_data_time: 0.0026   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:33:31 d2.utils.events]: \u001B[0m eta: 0:17:12  iter: 2079  total_loss: 0.3053  loss_cls: 0.03476  loss_box_reg: 0.0727  loss_mask: 0.1745  loss_rpn_cls: 0.001057  loss_rpn_loc: 0.004271    time: 1.0814  last_time: 0.6098  data_time: 0.0033  last_data_time: 0.0035   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:33:53 d2.utils.events]: \u001B[0m eta: 0:16:48  iter: 2099  total_loss: 0.3016  loss_cls: 0.03188  loss_box_reg: 0.07401  loss_mask: 0.1674  loss_rpn_cls: 0.001052  loss_rpn_loc: 0.004006    time: 1.0818  last_time: 1.1044  data_time: 0.0033  last_data_time: 0.0031   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:34:15 d2.utils.events]: \u001B[0m eta: 0:16:25  iter: 2119  total_loss: 0.2534  loss_cls: 0.03124  loss_box_reg: 0.0666  loss_mask: 0.1508  loss_rpn_cls: 0.0007047  loss_rpn_loc: 0.004059    time: 1.0816  last_time: 1.0323  data_time: 0.0032  last_data_time: 0.0048   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:34:36 d2.utils.events]: \u001B[0m eta: 0:16:04  iter: 2139  total_loss: 0.2722  loss_cls: 0.03021  loss_box_reg: 0.07303  loss_mask: 0.1563  loss_rpn_cls: 0.0005662  loss_rpn_loc: 0.003784    time: 1.0817  last_time: 1.1563  data_time: 0.0032  last_data_time: 0.0044   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:34:58 d2.utils.events]: \u001B[0m eta: 0:15:43  iter: 2159  total_loss: 0.3051  loss_cls: 0.03452  loss_box_reg: 0.06791  loss_mask: 0.182  loss_rpn_cls: 0.001059  loss_rpn_loc: 0.00435    time: 1.0815  last_time: 1.1760  data_time: 0.0031  last_data_time: 0.0044   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:35:18 d2.utils.events]: \u001B[0m eta: 0:15:18  iter: 2179  total_loss: 0.3043  loss_cls: 0.03193  loss_box_reg: 0.06303  loss_mask: 0.1838  loss_rpn_cls: 0.0012  loss_rpn_loc: 0.004652    time: 1.0808  last_time: 0.7057  data_time: 0.0031  last_data_time: 0.0028   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:35:39 d2.utils.events]: \u001B[0m eta: 0:14:58  iter: 2199  total_loss: 0.2619  loss_cls: 0.02646  loss_box_reg: 0.06473  loss_mask: 0.1673  loss_rpn_cls: 0.0004332  loss_rpn_loc: 0.003862    time: 1.0806  last_time: 1.2120  data_time: 0.0030  last_data_time: 0.0025   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:36:01 d2.utils.events]: \u001B[0m eta: 0:14:36  iter: 2219  total_loss: 0.2881  loss_cls: 0.03101  loss_box_reg: 0.07304  loss_mask: 0.167  loss_rpn_cls: 0.0004562  loss_rpn_loc: 0.003355    time: 1.0810  last_time: 1.1663  data_time: 0.0034  last_data_time: 0.0035   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:36:25 d2.utils.events]: \u001B[0m eta: 0:14:15  iter: 2239  total_loss: 0.272  loss_cls: 0.02893  loss_box_reg: 0.06632  loss_mask: 0.1611  loss_rpn_cls: 0.0005099  loss_rpn_loc: 0.004023    time: 1.0818  last_time: 1.0748  data_time: 0.0031  last_data_time: 0.0024   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:36:46 d2.utils.events]: \u001B[0m eta: 0:13:52  iter: 2259  total_loss: 0.2507  loss_cls: 0.03223  loss_box_reg: 0.06404  loss_mask: 0.1483  loss_rpn_cls: 0.0004453  loss_rpn_loc: 0.003995    time: 1.0815  last_time: 1.1912  data_time: 0.0030  last_data_time: 0.0029   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:37:08 d2.utils.events]: \u001B[0m eta: 0:13:30  iter: 2279  total_loss: 0.2399  loss_cls: 0.026  loss_box_reg: 0.05795  loss_mask: 0.1434  loss_rpn_cls: 0.0007548  loss_rpn_loc: 0.004056    time: 1.0820  last_time: 1.1536  data_time: 0.0031  last_data_time: 0.0025   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:37:30 d2.utils.events]: \u001B[0m eta: 0:13:07  iter: 2299  total_loss: 0.2569  loss_cls: 0.02665  loss_box_reg: 0.06305  loss_mask: 0.1501  loss_rpn_cls: 0.00041  loss_rpn_loc: 0.003367    time: 1.0819  last_time: 1.1855  data_time: 0.0030  last_data_time: 0.0033   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:37:51 d2.utils.events]: \u001B[0m eta: 0:12:44  iter: 2319  total_loss: 0.3174  loss_cls: 0.03475  loss_box_reg: 0.07645  loss_mask: 0.2  loss_rpn_cls: 0.001496  loss_rpn_loc: 0.004246    time: 1.0817  last_time: 1.1616  data_time: 0.0032  last_data_time: 0.0034   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:38:13 d2.utils.events]: \u001B[0m eta: 0:12:20  iter: 2339  total_loss: 0.2731  loss_cls: 0.0267  loss_box_reg: 0.06542  loss_mask: 0.1553  loss_rpn_cls: 0.0007412  loss_rpn_loc: 0.003679    time: 1.0818  last_time: 1.1441  data_time: 0.0032  last_data_time: 0.0035   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:38:36 d2.utils.events]: \u001B[0m eta: 0:12:00  iter: 2359  total_loss: 0.3006  loss_cls: 0.03045  loss_box_reg: 0.07021  loss_mask: 0.2005  loss_rpn_cls: 0.0006848  loss_rpn_loc: 0.003772    time: 1.0823  last_time: 1.1784  data_time: 0.0032  last_data_time: 0.0036   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:38:59 d2.utils.events]: \u001B[0m eta: 0:11:37  iter: 2379  total_loss: 0.2459  loss_cls: 0.02697  loss_box_reg: 0.05578  loss_mask: 0.1336  loss_rpn_cls: 0.00102  loss_rpn_loc: 0.003441    time: 1.0830  last_time: 1.0599  data_time: 0.0031  last_data_time: 0.0037   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/09 02:39:21 d2.data.datasets.coco]: \u001B[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001B[32m[07/09 02:39:21 d2.data.datasets.coco]: \u001B[0mLoaded 54 images in COCO format from C:\\Users\\Daanish Mittal\\OneDrive\\Desktop\\Tank_align\\tank_alignment\\army_ramp\\loader.ramp-2\\test\\_annotations.coco.json\n",
      "\u001B[32m[07/09 02:39:21 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001B[32m[07/09 02:39:21 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/09 02:39:21 d2.data.common]: \u001B[0mSerializing 54 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/09 02:39:21 d2.data.common]: \u001B[0mSerialized dataset takes 0.04 MiB\n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/09 02:39:21 d2.engine.defaults]: \u001B[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001B[32m[07/09 02:39:21 d2.utils.events]: \u001B[0m eta: 0:11:15  iter: 2399  total_loss: 0.2604  loss_cls: 0.02521  loss_box_reg: 0.06241  loss_mask: 0.1545  loss_rpn_cls: 0.0003382  loss_rpn_loc: 0.003734    time: 1.0832  last_time: 1.0721  data_time: 0.0030  last_data_time: 0.0032   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:39:43 d2.utils.events]: \u001B[0m eta: 0:10:53  iter: 2419  total_loss: 0.2859  loss_cls: 0.03475  loss_box_reg: 0.07541  loss_mask: 0.1732  loss_rpn_cls: 0.0007663  loss_rpn_loc: 0.004195    time: 1.0831  last_time: 0.6961  data_time: 0.0030  last_data_time: 0.0035   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:40:05 d2.utils.events]: \u001B[0m eta: 0:10:30  iter: 2439  total_loss: 0.2659  loss_cls: 0.02946  loss_box_reg: 0.06706  loss_mask: 0.1612  loss_rpn_cls: 0.001061  loss_rpn_loc: 0.004063    time: 1.0834  last_time: 1.1414  data_time: 0.0032  last_data_time: 0.0027   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:40:27 d2.utils.events]: \u001B[0m eta: 0:10:07  iter: 2459  total_loss: 0.241  loss_cls: 0.02719  loss_box_reg: 0.06267  loss_mask: 0.1513  loss_rpn_cls: 0.0006246  loss_rpn_loc: 0.003991    time: 1.0834  last_time: 1.2001  data_time: 0.0034  last_data_time: 0.0030   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:40:48 d2.utils.events]: \u001B[0m eta: 0:09:45  iter: 2479  total_loss: 0.2986  loss_cls: 0.03022  loss_box_reg: 0.07701  loss_mask: 0.1938  loss_rpn_cls: 0.0007727  loss_rpn_loc: 0.003854    time: 1.0833  last_time: 1.1350  data_time: 0.0030  last_data_time: 0.0040   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:41:11 d2.utils.events]: \u001B[0m eta: 0:09:24  iter: 2499  total_loss: 0.265  loss_cls: 0.02621  loss_box_reg: 0.05943  loss_mask: 0.1526  loss_rpn_cls: 0.0004571  loss_rpn_loc: 0.003558    time: 1.0838  last_time: 1.1741  data_time: 0.0031  last_data_time: 0.0036   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:41:33 d2.utils.events]: \u001B[0m eta: 0:09:02  iter: 2519  total_loss: 0.2252  loss_cls: 0.02554  loss_box_reg: 0.06012  loss_mask: 0.1353  loss_rpn_cls: 0.0004593  loss_rpn_loc: 0.003456    time: 1.0839  last_time: 0.6272  data_time: 0.0031  last_data_time: 0.0029   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:41:55 d2.utils.events]: \u001B[0m eta: 0:08:40  iter: 2539  total_loss: 0.2979  loss_cls: 0.03447  loss_box_reg: 0.07502  loss_mask: 0.187  loss_rpn_cls: 0.0004711  loss_rpn_loc: 0.004751    time: 1.0839  last_time: 1.2005  data_time: 0.0031  last_data_time: 0.0033   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:42:15 d2.utils.events]: \u001B[0m eta: 0:08:17  iter: 2559  total_loss: 0.2519  loss_cls: 0.02515  loss_box_reg: 0.05942  loss_mask: 0.156  loss_rpn_cls: 0.0004976  loss_rpn_loc: 0.003776    time: 1.0835  last_time: 0.7182  data_time: 0.0031  last_data_time: 0.0026   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:42:37 d2.utils.events]: \u001B[0m eta: 0:07:55  iter: 2579  total_loss: 0.2561  loss_cls: 0.02241  loss_box_reg: 0.05835  loss_mask: 0.1601  loss_rpn_cls: 0.0003383  loss_rpn_loc: 0.004159    time: 1.0835  last_time: 1.1665  data_time: 0.0030  last_data_time: 0.0024   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:42:56 d2.utils.events]: \u001B[0m eta: 0:07:32  iter: 2599  total_loss: 0.2324  loss_cls: 0.02538  loss_box_reg: 0.0558  loss_mask: 0.1438  loss_rpn_cls: 0.000843  loss_rpn_loc: 0.003977    time: 1.0825  last_time: 1.1449  data_time: 0.0031  last_data_time: 0.0035   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:43:17 d2.utils.events]: \u001B[0m eta: 0:07:09  iter: 2619  total_loss: 0.2886  loss_cls: 0.02808  loss_box_reg: 0.06647  loss_mask: 0.1731  loss_rpn_cls: 0.001377  loss_rpn_loc: 0.004    time: 1.0824  last_time: 1.0500  data_time: 0.0031  last_data_time: 0.0028   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:43:40 d2.utils.events]: \u001B[0m eta: 0:06:47  iter: 2639  total_loss: 0.2744  loss_cls: 0.03081  loss_box_reg: 0.0674  loss_mask: 0.1706  loss_rpn_cls: 0.0005946  loss_rpn_loc: 0.003358    time: 1.0829  last_time: 1.1532  data_time: 0.0030  last_data_time: 0.0033   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:44:01 d2.utils.events]: \u001B[0m eta: 0:06:24  iter: 2659  total_loss: 0.2945  loss_cls: 0.03253  loss_box_reg: 0.06905  loss_mask: 0.1749  loss_rpn_cls: 0.0008884  loss_rpn_loc: 0.004051    time: 1.0824  last_time: 1.2624  data_time: 0.0031  last_data_time: 0.0031   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:44:23 d2.utils.events]: \u001B[0m eta: 0:06:01  iter: 2679  total_loss: 0.2725  loss_cls: 0.03018  loss_box_reg: 0.06954  loss_mask: 0.1734  loss_rpn_cls: 0.0003801  loss_rpn_loc: 0.004449    time: 1.0827  last_time: 1.1994  data_time: 0.0032  last_data_time: 0.0026   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:44:45 d2.utils.events]: \u001B[0m eta: 0:05:38  iter: 2699  total_loss: 0.2532  loss_cls: 0.02767  loss_box_reg: 0.06341  loss_mask: 0.1434  loss_rpn_cls: 0.0009405  loss_rpn_loc: 0.003796    time: 1.0828  last_time: 1.2024  data_time: 0.0035  last_data_time: 0.0027   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:45:07 d2.utils.events]: \u001B[0m eta: 0:05:16  iter: 2719  total_loss: 0.2742  loss_cls: 0.03086  loss_box_reg: 0.06532  loss_mask: 0.1592  loss_rpn_cls: 0.0003564  loss_rpn_loc: 0.003755    time: 1.0830  last_time: 1.3256  data_time: 0.0032  last_data_time: 0.0030   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:45:27 d2.utils.events]: \u001B[0m eta: 0:04:53  iter: 2739  total_loss: 0.2628  loss_cls: 0.03355  loss_box_reg: 0.06547  loss_mask: 0.149  loss_rpn_cls: 0.0005407  loss_rpn_loc: 0.004245    time: 1.0823  last_time: 0.6137  data_time: 0.0031  last_data_time: 0.0028   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:45:48 d2.utils.events]: \u001B[0m eta: 0:04:31  iter: 2759  total_loss: 0.229  loss_cls: 0.02833  loss_box_reg: 0.0556  loss_mask: 0.1378  loss_rpn_cls: 0.0003809  loss_rpn_loc: 0.003713    time: 1.0820  last_time: 0.7132  data_time: 0.0031  last_data_time: 0.0032   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:46:09 d2.utils.events]: \u001B[0m eta: 0:04:09  iter: 2779  total_loss: 0.3005  loss_cls: 0.02917  loss_box_reg: 0.0701  loss_mask: 0.1621  loss_rpn_cls: 0.001009  loss_rpn_loc: 0.003601    time: 1.0817  last_time: 1.1867  data_time: 0.0030  last_data_time: 0.0029   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/09 02:46:30 d2.data.datasets.coco]: \u001B[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001B[32m[07/09 02:46:30 d2.data.datasets.coco]: \u001B[0mLoaded 54 images in COCO format from C:\\Users\\Daanish Mittal\\OneDrive\\Desktop\\Tank_align\\tank_alignment\\army_ramp\\loader.ramp-2\\test\\_annotations.coco.json\n",
      "\u001B[32m[07/09 02:46:30 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001B[32m[07/09 02:46:30 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/09 02:46:30 d2.data.common]: \u001B[0mSerializing 54 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/09 02:46:30 d2.data.common]: \u001B[0mSerialized dataset takes 0.04 MiB\n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/09 02:46:30 d2.engine.defaults]: \u001B[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001B[32m[07/09 02:46:30 d2.utils.events]: \u001B[0m eta: 0:03:46  iter: 2799  total_loss: 0.2577  loss_cls: 0.02852  loss_box_reg: 0.06196  loss_mask: 0.1643  loss_rpn_cls: 0.0006112  loss_rpn_loc: 0.004347    time: 1.0815  last_time: 0.5859  data_time: 0.0029  last_data_time: 0.0025   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:46:49 d2.utils.events]: \u001B[0m eta: 0:03:23  iter: 2819  total_loss: 0.2528  loss_cls: 0.02711  loss_box_reg: 0.06334  loss_mask: 0.1524  loss_rpn_cls: 0.0005305  loss_rpn_loc: 0.003829    time: 1.0806  last_time: 1.0714  data_time: 0.0032  last_data_time: 0.0028   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:47:10 d2.utils.events]: \u001B[0m eta: 0:03:00  iter: 2839  total_loss: 0.2669  loss_cls: 0.02971  loss_box_reg: 0.0634  loss_mask: 0.1575  loss_rpn_cls: 0.0007503  loss_rpn_loc: 0.004184    time: 1.0804  last_time: 1.1373  data_time: 0.0030  last_data_time: 0.0027   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:47:32 d2.utils.events]: \u001B[0m eta: 0:02:37  iter: 2859  total_loss: 0.2485  loss_cls: 0.02676  loss_box_reg: 0.06414  loss_mask: 0.1516  loss_rpn_cls: 0.0003488  loss_rpn_loc: 0.003653    time: 1.0804  last_time: 1.0595  data_time: 0.0032  last_data_time: 0.0030   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:47:53 d2.utils.events]: \u001B[0m eta: 0:02:15  iter: 2879  total_loss: 0.2355  loss_cls: 0.02551  loss_box_reg: 0.06562  loss_mask: 0.1442  loss_rpn_cls: 0.0006966  loss_rpn_loc: 0.004025    time: 1.0804  last_time: 1.1815  data_time: 0.0032  last_data_time: 0.0030   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:48:15 d2.utils.events]: \u001B[0m eta: 0:01:52  iter: 2899  total_loss: 0.2945  loss_cls: 0.03159  loss_box_reg: 0.0733  loss_mask: 0.1523  loss_rpn_cls: 0.0002237  loss_rpn_loc: 0.003783    time: 1.0803  last_time: 1.0781  data_time: 0.0030  last_data_time: 0.0030   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:48:37 d2.utils.events]: \u001B[0m eta: 0:01:30  iter: 2919  total_loss: 0.2703  loss_cls: 0.02845  loss_box_reg: 0.06965  loss_mask: 0.1631  loss_rpn_cls: 0.0003848  loss_rpn_loc: 0.003702    time: 1.0804  last_time: 1.0619  data_time: 0.0031  last_data_time: 0.0026   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:49:00 d2.utils.events]: \u001B[0m eta: 0:01:07  iter: 2939  total_loss: 0.2512  loss_cls: 0.02665  loss_box_reg: 0.06586  loss_mask: 0.1532  loss_rpn_cls: 0.0004556  loss_rpn_loc: 0.003364    time: 1.0810  last_time: 1.0684  data_time: 0.0031  last_data_time: 0.0031   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:49:22 d2.utils.events]: \u001B[0m eta: 0:00:45  iter: 2959  total_loss: 0.251  loss_cls: 0.02471  loss_box_reg: 0.06364  loss_mask: 0.1631  loss_rpn_cls: 0.0004363  loss_rpn_loc: 0.003777    time: 1.0810  last_time: 1.0564  data_time: 0.0030  last_data_time: 0.0032   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:49:43 d2.utils.events]: \u001B[0m eta: 0:00:22  iter: 2979  total_loss: 0.2691  loss_cls: 0.02824  loss_box_reg: 0.06315  loss_mask: 0.1599  loss_rpn_cls: 0.0006416  loss_rpn_loc: 0.003949    time: 1.0810  last_time: 1.2061  data_time: 0.0031  last_data_time: 0.0022   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:50:05 d2.utils.events]: \u001B[0m eta: 0:00:00  iter: 2999  total_loss: 0.2641  loss_cls: 0.02462  loss_box_reg: 0.06252  loss_mask: 0.1629  loss_rpn_cls: 0.0004991  loss_rpn_loc: 0.003571    time: 1.0809  last_time: 0.6866  data_time: 0.0031  last_data_time: 0.0034   lr: 0.0001  max_mem: 5407M\n",
      "\u001B[32m[07/09 02:50:05 d2.engine.hooks]: \u001B[0mOverall training speed: 2998 iterations in 0:54:00 (1.0809 s / it)\n",
      "\u001B[32m[07/09 02:50:05 d2.engine.hooks]: \u001B[0mTotal training time: 0:54:07 (0:00:06 on hooks)\n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/09 02:50:05 d2.data.datasets.coco]: \u001B[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001B[32m[07/09 02:50:05 d2.data.datasets.coco]: \u001B[0mLoaded 54 images in COCO format from C:\\Users\\Daanish Mittal\\OneDrive\\Desktop\\Tank_align\\tank_alignment\\army_ramp\\loader.ramp-2\\test\\_annotations.coco.json\n",
      "\u001B[32m[07/09 02:50:05 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001B[32m[07/09 02:50:05 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/09 02:50:05 d2.data.common]: \u001B[0mSerializing 54 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/09 02:50:05 d2.data.common]: \u001B[0mSerialized dataset takes 0.04 MiB\n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/09 02:50:05 d2.engine.defaults]: \u001B[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2XMPKQ28GRna"
   },
   "outputs": [],
   "source": [
    "# Look at training curves in tensorboard:\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir $OUTPUT_DIR_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flInE1L-XTfx"
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2265,
     "status": "ok",
     "timestamp": 1668005909534,
     "user": {
      "displayName": "Piotr Skalski",
      "userId": "04309230031174164349"
     },
     "user_tz": -60
    },
    "id": "vsByFDFbQwLi",
    "outputId": "db8479c0-a286-4e83-e064-9ae4ebad3aba",
    "ExecuteTime": {
     "end_time": "2024-07-09T04:32:24.385016Z",
     "start_time": "2024-07-09T04:32:21.644408Z"
    }
   },
   "source": [
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7\n",
    "predictor = DefaultPredictor(cfg)"
   ],
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m cfg\u001B[38;5;241m.\u001B[39mMODEL\u001B[38;5;241m.\u001B[39mWEIGHTS \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(cfg\u001B[38;5;241m.\u001B[39mOUTPUT_DIR, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_final.pth\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      2\u001B[0m cfg\u001B[38;5;241m.\u001B[39mMODEL\u001B[38;5;241m.\u001B[39mROI_HEADS\u001B[38;5;241m.\u001B[39mSCORE_THRESH_TEST \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.7\u001B[39m\n\u001B[1;32m----> 3\u001B[0m predictor \u001B[38;5;241m=\u001B[39m \u001B[43mDefaultPredictor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcfg\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\detectron2\\engine\\defaults.py:282\u001B[0m, in \u001B[0;36mDefaultPredictor.__init__\u001B[1;34m(self, cfg)\u001B[0m\n\u001B[0;32m    280\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, cfg):\n\u001B[0;32m    281\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcfg \u001B[38;5;241m=\u001B[39m cfg\u001B[38;5;241m.\u001B[39mclone()  \u001B[38;5;66;03m# cfg can be modified by model\u001B[39;00m\n\u001B[1;32m--> 282\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m=\u001B[39m \u001B[43mbuild_model\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcfg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    283\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39meval()\n\u001B[0;32m    284\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(cfg\u001B[38;5;241m.\u001B[39mDATASETS\u001B[38;5;241m.\u001B[39mTEST):\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\detectron2\\modeling\\meta_arch\\build.py:23\u001B[0m, in \u001B[0;36mbuild_model\u001B[1;34m(cfg)\u001B[0m\n\u001B[0;32m     21\u001B[0m meta_arch \u001B[38;5;241m=\u001B[39m cfg\u001B[38;5;241m.\u001B[39mMODEL\u001B[38;5;241m.\u001B[39mMETA_ARCHITECTURE\n\u001B[0;32m     22\u001B[0m model \u001B[38;5;241m=\u001B[39m META_ARCH_REGISTRY\u001B[38;5;241m.\u001B[39mget(meta_arch)(cfg)\n\u001B[1;32m---> 23\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcfg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mMODEL\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDEVICE\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     24\u001B[0m _log_api_usage(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodeling.meta_arch.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m meta_arch)\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m model\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1145\u001B[0m, in \u001B[0;36mModule.to\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1141\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1142\u001B[0m                     non_blocking, memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format)\n\u001B[0;32m   1143\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, non_blocking)\n\u001B[1;32m-> 1145\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn)\u001B[0m\n\u001B[0;32m    795\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_apply\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn):\n\u001B[0;32m    796\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[1;32m--> 797\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    799\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[0;32m    800\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[0;32m    801\u001B[0m             \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[0;32m    802\u001B[0m             \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    807\u001B[0m             \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[0;32m    808\u001B[0m             \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn)\u001B[0m\n\u001B[0;32m    795\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_apply\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn):\n\u001B[0;32m    796\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[1;32m--> 797\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    799\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[0;32m    800\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[0;32m    801\u001B[0m             \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[0;32m    802\u001B[0m             \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    807\u001B[0m             \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[0;32m    808\u001B[0m             \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:820\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn)\u001B[0m\n\u001B[0;32m    816\u001B[0m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[0;32m    817\u001B[0m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[0;32m    818\u001B[0m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[0;32m    819\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m--> 820\u001B[0m     param_applied \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    821\u001B[0m should_use_set_data \u001B[38;5;241m=\u001B[39m compute_should_use_set_data(param, param_applied)\n\u001B[0;32m    822\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m should_use_set_data:\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1143\u001B[0m, in \u001B[0;36mModule.to.<locals>.convert\u001B[1;34m(t)\u001B[0m\n\u001B[0;32m   1140\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m convert_to_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m t\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m):\n\u001B[0;32m   1141\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1142\u001B[0m                 non_blocking, memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format)\n\u001B[1;32m-> 1143\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_floating_point\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_complex\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnon_blocking\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "hmAcBbpXX-Rh",
    "ExecuteTime": {
     "end_time": "2024-07-09T04:32:36.203721Z",
     "start_time": "2024-07-09T04:32:36.143314Z"
    }
   },
   "source": [
    "dataset_valid = DatasetCatalog.get(VALID_DATA_SET_NAME)\n",
    "\n",
    "for d in dataset_valid:\n",
    "    img = cv2.imread(d[\"file_name\"])\n",
    "    outputs = predictor(img)\n",
    "\n",
    "    visualizer = Visualizer(\n",
    "\n",
    "        img[:, :, ::-1],\n",
    "        metadata=metadata,\n",
    "        scale=0.5,\n",
    "        instance_mode=ColorMode.IMAGE_BW\n",
    "    )\n",
    "    out = visualizer.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    cv2.imshow(\"validation\",out.get_image()[:, :, ::-1])\n",
    "    cv2.waitKey(0)  # Waits for a key press to close the window\n",
    "    cv2.destroyAllWindows()  # Closes the window"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/09 10:02:36 d2.data.datasets.coco]: \u001B[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001B[32m[07/09 10:02:36 d2.data.datasets.coco]: \u001B[0mLoaded 107 images in COCO format from C:\\Users\\Daanish Mittal\\OneDrive\\Desktop\\Tank_align\\tank_alignment\\army_ramp\\loader.ramp-2\\valid\\_annotations.coco.json\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'predictor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m dataset_valid:\n\u001B[0;32m      4\u001B[0m     img \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mimread(d[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfile_name\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m----> 5\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mpredictor\u001B[49m(img)\n\u001B[0;32m      7\u001B[0m     visualizer \u001B[38;5;241m=\u001B[39m Visualizer(\n\u001B[0;32m      8\u001B[0m \n\u001B[0;32m      9\u001B[0m         img[:, :, ::\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     12\u001B[0m         instance_mode\u001B[38;5;241m=\u001B[39mColorMode\u001B[38;5;241m.\u001B[39mIMAGE_BW\n\u001B[0;32m     13\u001B[0m     )\n\u001B[0;32m     14\u001B[0m     out \u001B[38;5;241m=\u001B[39m visualizer\u001B[38;5;241m.\u001B[39mdraw_instance_predictions(outputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minstances\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
      "\u001B[1;31mNameError\u001B[0m: name 'predictor' is not defined"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T05:02:57.692337Z",
     "start_time": "2024-07-09T05:02:21.324168Z"
    }
   },
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "\n",
    "def load_model(config_file, weights_file):\n",
    "    # Load the configuration\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(config_file)\n",
    "    cfg.MODEL.WEIGHTS = weights_file\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.96  # Set a custom testing threshold\n",
    "    predictor = DefaultPredictor(cfg)\n",
    "    \n",
    "    # Print device information\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Using GPU:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "    else:\n",
    "        print(\"Using CPU\")\n",
    "    \n",
    "    return predictor\n",
    "\n",
    "def process_image(predictor, image_path):\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "    outputs = predictor(image)\n",
    "    \n",
    "    # Visualize the results\n",
    "    v = Visualizer(image[:, :, ::-1], MetadataCatalog.get(predictor.cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    \n",
    "    # Display the image\n",
    "    result_image = out.get_image()[:, :, ::-1]\n",
    "    cv2.imshow(f'Result for {image_path}', result_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def process_video(predictor, video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        outputs = predictor(frame)\n",
    "        v = Visualizer(frame[:, :, ::-1], MetadataCatalog.get(predictor.cfg.DATASETS.TRAIN[0]), scale=0.45)\n",
    "        out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "        \n",
    "        # Display the frame\n",
    "        result_frame = out.get_image()[:, :, ::-1]\n",
    "        cv2.imshow(f'Result for a frame in {video_path}', result_frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def process_folder(predictor, folder_path):\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            if file_path.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff')):\n",
    "                process_image(predictor, file_path)\n",
    "            elif file_path.lower().endswith(('.mp4', '.avi', '.mov', '.mkv')):\n",
    "                process_video(predictor, file_path)\n",
    "\n",
    "# Example usage\n",
    "config_file = \"C:/Users/Daanish Mittal/OneDrive/Desktop/Tank_align/tank_alignment/army_ramp/loader.ramp/mask_rcnn_R_101_FPN_3x/2024-07-09-01-55-42/config.yaml\"\n",
    "weights_file = \"C:/Users/Daanish Mittal/OneDrive/Desktop/Tank_align/tank_alignment/army_ramp/loader.ramp/mask_rcnn_R_101_FPN_3x/2024-07-09-01-55-42/model_final.pth\"\n",
    "input_folder = \"C:/Users/Daanish Mittal/OneDrive/Desktop/Tank_align/tank_alignment/test_army\"\n",
    "\n",
    "predictor = load_model(config_file, weights_file)\n",
    "process_folder(predictor, input_folder)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "import numpy as np\n",
    "\n",
    "def load_model(config_file, weights_file):\n",
    "    # Load the configuration\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(config_file)\n",
    "    cfg.MODEL.WEIGHTS = weights_file\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.95  # Set a custom testing threshold\n",
    "    return DefaultPredictor(cfg)\n",
    "\n",
    "def draw_quadrilaterals(image, outputs):\n",
    "    masks = outputs[\"instances\"].pred_masks.to(\"cpu\").numpy()\n",
    "    for mask in masks:\n",
    "        # Find contours\n",
    "        contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        if len(contours) == 0:\n",
    "            continue\n",
    "        \n",
    "        for contour in contours:\n",
    "            # Approximate the contour to a quadrilateral\n",
    "            epsilon = 0.02 * cv2.arcLength(contour, True)\n",
    "            approx = cv2.approxPolyDP(contour, epsilon, True)\n",
    "            \n",
    "            # Calculate the area of the mask and the quadrilateral\n",
    "            mask_area = cv2.contourArea(contour)\n",
    "            quad_area = cv2.contourArea(approx)\n",
    "            \n",
    "            # Check if mask_area is zero (handle edge case)\n",
    "            if mask_area <= 0:\n",
    "                continue\n",
    "            \n",
    "            # Calculate the percentage of the mask area outside the quadrilateral\n",
    "            outside_area = mask_area - quad_area\n",
    "            outside_percentage = outside_area / mask_area if mask_area > 0 else 0\n",
    "            \n",
    "            # Determine the color of the quadrilateral\n",
    "            color = (0, 255, 0) if outside_percentage <= 0.12 else (0, 0, 255)\n",
    "            \n",
    "            if len(approx) >= 4:\n",
    "                cv2.polylines(image, [approx], True, color, 2)\n",
    "                \n",
    "    return image\n",
    "\n",
    "\n",
    "def process_image(predictor, image_path, output_folder):\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "    outputs = predictor(image)\n",
    "    \n",
    "    # Draw quadrilaterals on the image\n",
    "    result_image = draw_quadrilaterals(image, outputs)\n",
    "    \n",
    "    # Save the result image\n",
    "    output_path = os.path.join(output_folder, os.path.basename(image_path))\n",
    "    cv2.imwrite(output_path, result_image)\n",
    "    print(f'Saved result for {image_path} to {output_path}')\n",
    "\n",
    "def process_video(predictor, video_path, output_folder):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    output_path = os.path.join(output_folder, os.path.basename(video_path))\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        outputs = predictor(frame)\n",
    "        result_frame = draw_quadrilaterals(frame, outputs)\n",
    "        \n",
    "        # Write the frame to the output video\n",
    "        out.write(result_frame)\n",
    "    \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f'Saved result for {video_path} to {output_path}')\n",
    "\n",
    "def process_folder(predictor, folder_path, output_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            if file_path.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff')):\n",
    "                process_image(predictor, file_path, output_folder)\n",
    "            elif file_path.lower().endswith(('.mp4', '.avi', '.mov', '.mkv')):\n",
    "                process_video(predictor, file_path, output_folder)\n",
    "\n",
    "# Example usage\n",
    "config_file = \"D:/path_detect/tank/mask_rcnn_R_101_FPN_3x/2024-06-25-16-10-19/config.yaml\"\n",
    "weights_file = \"D:/path_detect/tank/mask_rcnn_R_101_FPN_3x/2024-06-25-16-10-19/model_final.pth\"\n",
    "\n",
    "input_folder = \"D:/path_detect/test\"\n",
    "output_folder = \"D:/path_detect/output\"\n",
    "\n",
    "predictor = load_model(config_file, weights_file)\n",
    "process_folder(predictor, input_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def detect_ramp_edges(image):\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply GaussianBlur to reduce noise and improve edge detection\n",
    "    blurred = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "\n",
    "    # Detect edges using Canny\n",
    "    edges = cv2.Canny(blurred, 300, 250, apertureSize=3)\n",
    "\n",
    "    # Create a mask to remove boundary edges\n",
    "    height, width = edges.shape\n",
    "    mask = np.ones_like(edges)\n",
    "    border = 10  # Adjust this value to change the border size\n",
    "    mask[:border, :] = 0\n",
    "    mask[-border:, :] = 0\n",
    "    mask[:, :border] = 0\n",
    "    mask[:, -border:] = 0\n",
    "\n",
    "    # Apply the mask to remove boundary edges\n",
    "    edges = cv2.bitwise_and(edges, edges, mask=mask)\n",
    "\n",
    "\n",
    "    # Detect lines using Hough Transform\n",
    "    lines = cv2.HoughLines(edges, 1.1, np.pi / 180,200)\n",
    "\n",
    "\n",
    "    return lines\n",
    "\n",
    "def draw_lines(image, lines):\n",
    "    # Draw the lines on the image\n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            rho, theta = line[0]\n",
    "            a = np.cos(theta)\n",
    "            b = np.sin(theta)\n",
    "            x0 = a * rho\n",
    "            y0 = b * rho\n",
    "            x1 = int(x0 + 1000 * (-b))\n",
    "            y1 = int(y0 + 1000 * (a))\n",
    "            x2 = int(x0 - 1000 * (-b))\n",
    "            y2 = int(y0 - 1000 * (a))\n",
    "            cv2.line(image, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "    return image\n",
    "\n",
    "def process_video(video_source=0):\n",
    "    # Open the video source (0 for webcam, or a filename)\n",
    "    cap = cv2.VideoCapture(video_source)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video source.\")\n",
    "        return\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Detect ramp edges\n",
    "        lines = detect_ramp_edges(frame)\n",
    "\n",
    "        # Draw detected lines on the frame\n",
    "        frame_with_lines = draw_lines(frame, lines)\n",
    "\n",
    "        # Display the result\n",
    "        cv2.imshow('Detected Ramp Edges', frame_with_lines)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_video('D:/path_detect/video1.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def detect_ramp_edges(image):\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply GaussianBlur to reduce noise and improve edge detection\n",
    "    blurred = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "\n",
    "    # Detect edges using Canny\n",
    "    edges = cv2.Canny(blurred, 100, 200, apertureSize=3)\n",
    "\n",
    "    # Create a mask to remove boundary edges\n",
    "    height, width = edges.shape\n",
    "    mask = np.ones_like(edges)\n",
    "    border = 10  # Adjust this value to change the border size\n",
    "    mask[:border, :] = 0\n",
    "    mask[-border:, :] = 0\n",
    "    mask[:, :border] = 0\n",
    "    mask[:, -border:] = 0\n",
    "\n",
    "    # Apply the mask to remove boundary edges\n",
    "    edges = cv2.bitwise_and(edges, edges, mask=mask)\n",
    "\n",
    "    # Detect lines using Hough Transform\n",
    "    lines = cv2.HoughLines(edges, 1.1, np.pi / 180, 200)\n",
    "\n",
    "    return lines\n",
    "\n",
    "def draw_lines(image, lines):\n",
    "    # Draw the lines on the image\n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            rho, theta = line[0]\n",
    "            a = np.cos(theta)\n",
    "            b = np.sin(theta)\n",
    "            x0 = a * rho\n",
    "            y0 = b * rho\n",
    "            x1 = int(x0 + 1000 * (-b))\n",
    "            y1 = int(y0 + 1000 * (a))\n",
    "            x2 = int(x0 - 1000 * (-b))\n",
    "            y2 = int(y0 - 1000 * (a))\n",
    "            cv2.line(image, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "    return image\n",
    "\n",
    "def process_video(video_source=0):\n",
    "    # Open the video source (0 for webcam, or a filename)\n",
    "    cap = cv2.VideoCapture(video_source)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video source.\")\n",
    "        return\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Detect ramp edges\n",
    "        lines = detect_ramp_edges(frame)\n",
    "\n",
    "        # Draw detected lines on the frame\n",
    "        frame_with_lines = draw_lines(frame, lines)\n",
    "\n",
    "        # Display the result\n",
    "        cv2.imshow('Detected Ramp Edges', frame_with_lines)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_video()  # Default is the webcam, pass a video file path if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "1Wkuq4DLHD7MG5HgpEqiytvEjvNK2sYAx",
     "timestamp": 1719261218111
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
